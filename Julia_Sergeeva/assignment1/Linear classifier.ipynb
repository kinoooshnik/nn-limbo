{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "[3. 2.]\n",
      "[[3. 2.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([3.0]))\n",
    "print(np.array([3.0, 2.0]))\n",
    "print(np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1805.823781\n",
      "Epoch 1, loss: 1641.779170\n",
      "Epoch 2, loss: 1794.308734\n",
      "Epoch 3, loss: 1642.696400\n",
      "Epoch 4, loss: 1871.687321\n",
      "Epoch 5, loss: 1746.192947\n",
      "Epoch 6, loss: 1874.770394\n",
      "Epoch 7, loss: 2040.166872\n",
      "Epoch 8, loss: 1845.062881\n",
      "Epoch 9, loss: 1638.308932\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1152f8240>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 1902.093047\n",
      "Epoch 1, loss: 1511.693429\n",
      "Epoch 2, loss: 2012.360104\n",
      "Epoch 3, loss: 1724.233426\n",
      "Epoch 4, loss: 1949.359197\n",
      "Epoch 5, loss: 1564.904170\n",
      "Epoch 6, loss: 1445.012271\n",
      "Epoch 7, loss: 1786.417591\n",
      "Epoch 8, loss: 1662.964798\n",
      "Epoch 9, loss: 1695.275632\n",
      "Epoch 10, loss: 1670.344623\n",
      "Epoch 11, loss: 1754.713348\n",
      "Epoch 12, loss: 1913.610725\n",
      "Epoch 13, loss: 2024.452840\n",
      "Epoch 14, loss: 1733.338931\n",
      "Epoch 15, loss: 1576.519290\n",
      "Epoch 16, loss: 1818.469761\n",
      "Epoch 17, loss: 1528.489058\n",
      "Epoch 18, loss: 1797.725568\n",
      "Epoch 19, loss: 1655.904693\n",
      "Epoch 20, loss: 1574.237391\n",
      "Epoch 21, loss: 1882.358547\n",
      "Epoch 22, loss: 1592.014712\n",
      "Epoch 23, loss: 1662.061707\n",
      "Epoch 24, loss: 1423.312270\n",
      "Epoch 25, loss: 1843.464925\n",
      "Epoch 26, loss: 1836.957154\n",
      "Epoch 27, loss: 1732.986132\n",
      "Epoch 28, loss: 1970.752279\n",
      "Epoch 29, loss: 1785.082446\n",
      "Epoch 30, loss: 1458.307608\n",
      "Epoch 31, loss: 2105.059540\n",
      "Epoch 32, loss: 1749.964374\n",
      "Epoch 33, loss: 1699.902655\n",
      "Epoch 34, loss: 1796.279785\n",
      "Epoch 35, loss: 1737.842046\n",
      "Epoch 36, loss: 1624.196238\n",
      "Epoch 37, loss: 1694.980972\n",
      "Epoch 38, loss: 1937.814220\n",
      "Epoch 39, loss: 1757.319734\n",
      "Epoch 40, loss: 1486.509520\n",
      "Epoch 41, loss: 1949.880254\n",
      "Epoch 42, loss: 1440.210805\n",
      "Epoch 43, loss: 1494.923055\n",
      "Epoch 44, loss: 1777.702027\n",
      "Epoch 45, loss: 1748.706048\n",
      "Epoch 46, loss: 1819.876754\n",
      "Epoch 47, loss: 1452.723237\n",
      "Epoch 48, loss: 1800.003989\n",
      "Epoch 49, loss: 1952.628885\n",
      "Epoch 50, loss: 1744.913576\n",
      "Epoch 51, loss: 1661.265296\n",
      "Epoch 52, loss: 1622.465844\n",
      "Epoch 53, loss: 1665.963303\n",
      "Epoch 54, loss: 1699.801832\n",
      "Epoch 55, loss: 1671.047433\n",
      "Epoch 56, loss: 1670.973003\n",
      "Epoch 57, loss: 1555.106600\n",
      "Epoch 58, loss: 1816.016618\n",
      "Epoch 59, loss: 1766.639854\n",
      "Epoch 60, loss: 1541.904936\n",
      "Epoch 61, loss: 1811.170920\n",
      "Epoch 62, loss: 1768.129906\n",
      "Epoch 63, loss: 2149.555419\n",
      "Epoch 64, loss: 1588.838502\n",
      "Epoch 65, loss: 1845.545650\n",
      "Epoch 66, loss: 1636.602527\n",
      "Epoch 67, loss: 2294.352327\n",
      "Epoch 68, loss: 2019.998458\n",
      "Epoch 69, loss: 2158.117042\n",
      "Epoch 70, loss: 1813.945104\n",
      "Epoch 71, loss: 1559.153090\n",
      "Epoch 72, loss: 1585.583998\n",
      "Epoch 73, loss: 1450.440256\n",
      "Epoch 74, loss: 1593.968195\n",
      "Epoch 75, loss: 2170.331192\n",
      "Epoch 76, loss: 1439.950091\n",
      "Epoch 77, loss: 1994.673432\n",
      "Epoch 78, loss: 1406.778126\n",
      "Epoch 79, loss: 1908.803462\n",
      "Epoch 80, loss: 1791.385217\n",
      "Epoch 81, loss: 1714.204431\n",
      "Epoch 82, loss: 2297.975361\n",
      "Epoch 83, loss: 1791.361700\n",
      "Epoch 84, loss: 1879.206638\n",
      "Epoch 85, loss: 1576.510626\n",
      "Epoch 86, loss: 1647.067522\n",
      "Epoch 87, loss: 1426.495108\n",
      "Epoch 88, loss: 1725.728953\n",
      "Epoch 89, loss: 1845.894791\n",
      "Epoch 90, loss: 1446.859313\n",
      "Epoch 91, loss: 2001.080166\n",
      "Epoch 92, loss: 1883.730627\n",
      "Epoch 93, loss: 1833.629121\n",
      "Epoch 94, loss: 1761.009127\n",
      "Epoch 95, loss: 1950.650215\n",
      "Epoch 96, loss: 1732.452178\n",
      "Epoch 97, loss: 2004.700323\n",
      "Epoch 98, loss: 1661.096909\n",
      "Epoch 99, loss: 1627.877631\n",
      "Accuracy after training for 100 epochs:  0.146\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_val_accuracy = 0\n",
    "r_best = 0\n",
    "l_best = 0\n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=l, batch_size=batch_size, reg=r)\n",
    "        prediction = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(prediction, val_y)\n",
    "        \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            l_best, r_best = l, r\n",
    "            best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print('best param', l_best, r_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
