{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000) \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 3073)\n",
      "(9000,)\n",
      "(1000, 3073)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "print(probs)\n",
    "\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.006760443547122\n"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "# print(probs)\n",
    "print(linear_classifer.cross_entropy_loss(probs, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "#print(loss, grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 898.952293\n",
      "Epoch 1, loss: 816.194774\n",
      "Epoch 2, loss: 891.429815\n",
      "Epoch 3, loss: 814.898675\n",
      "Epoch 4, loss: 929.844823\n",
      "Epoch 5, loss: 865.632208\n",
      "Epoch 6, loss: 931.091506\n",
      "Epoch 7, loss: 1013.350902\n",
      "Epoch 8, loss: 914.784839\n",
      "Epoch 9, loss: 813.284659\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21d415ad040>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.15\n",
      "Epoch 0, loss: 944.903286\n",
      "Epoch 1, loss: 749.665666\n",
      "Epoch 2, loss: 998.808337\n",
      "Epoch 3, loss: 856.117335\n",
      "Epoch 4, loss: 967.788841\n",
      "Epoch 5, loss: 776.844931\n",
      "Epoch 6, loss: 715.805885\n",
      "Epoch 7, loss: 886.629020\n",
      "Epoch 8, loss: 825.677675\n",
      "Epoch 9, loss: 841.382084\n",
      "Epoch 10, loss: 829.201472\n",
      "Epoch 11, loss: 869.654826\n",
      "Epoch 12, loss: 948.022522\n",
      "Epoch 13, loss: 1005.674605\n",
      "Epoch 14, loss: 861.247777\n",
      "Epoch 15, loss: 782.456888\n",
      "Epoch 16, loss: 902.937625\n",
      "Epoch 17, loss: 757.488625\n",
      "Epoch 18, loss: 892.230149\n",
      "Epoch 19, loss: 822.012115\n",
      "Epoch 20, loss: 781.078026\n",
      "Epoch 21, loss: 934.127674\n",
      "Epoch 22, loss: 789.135894\n",
      "Epoch 23, loss: 824.718938\n",
      "Epoch 24, loss: 704.912313\n",
      "Epoch 25, loss: 915.871542\n",
      "Epoch 26, loss: 911.931770\n",
      "Epoch 27, loss: 860.948069\n",
      "Epoch 28, loss: 978.683723\n",
      "Epoch 29, loss: 885.967114\n",
      "Epoch 30, loss: 723.218318\n",
      "Epoch 31, loss: 1044.913209\n",
      "Epoch 32, loss: 866.798825\n",
      "Epoch 33, loss: 843.543127\n",
      "Epoch 34, loss: 891.900430\n",
      "Epoch 35, loss: 862.928630\n",
      "Epoch 36, loss: 806.273173\n",
      "Epoch 37, loss: 841.787247\n",
      "Epoch 38, loss: 962.310530\n",
      "Epoch 39, loss: 872.513305\n",
      "Epoch 40, loss: 736.988807\n",
      "Epoch 41, loss: 967.912493\n",
      "Epoch 42, loss: 714.131563\n",
      "Epoch 43, loss: 739.533045\n",
      "Epoch 44, loss: 882.713526\n",
      "Epoch 45, loss: 868.146164\n",
      "Epoch 46, loss: 903.303924\n",
      "Epoch 47, loss: 720.907867\n",
      "Epoch 48, loss: 893.109228\n",
      "Epoch 49, loss: 969.509150\n",
      "Epoch 50, loss: 866.007931\n",
      "Epoch 51, loss: 824.308712\n",
      "Epoch 52, loss: 805.104439\n",
      "Epoch 53, loss: 825.510941\n",
      "Epoch 54, loss: 843.480887\n",
      "Epoch 55, loss: 829.608360\n",
      "Epoch 56, loss: 827.944498\n",
      "Epoch 57, loss: 770.629551\n",
      "Epoch 58, loss: 901.718324\n",
      "Epoch 59, loss: 877.553312\n",
      "Epoch 60, loss: 764.389510\n",
      "Epoch 61, loss: 899.589369\n",
      "Epoch 62, loss: 877.123306\n",
      "Epoch 63, loss: 1068.022117\n",
      "Epoch 64, loss: 788.071035\n",
      "Epoch 65, loss: 916.234058\n",
      "Epoch 66, loss: 811.844016\n",
      "Epoch 67, loss: 1139.547653\n",
      "Epoch 68, loss: 1001.961812\n",
      "Epoch 69, loss: 1071.934420\n",
      "Epoch 70, loss: 900.803793\n",
      "Epoch 71, loss: 771.870998\n",
      "Epoch 72, loss: 786.335177\n",
      "Epoch 73, loss: 719.422272\n",
      "Epoch 74, loss: 791.161262\n",
      "Epoch 75, loss: 1077.612666\n",
      "Epoch 76, loss: 713.864637\n",
      "Epoch 77, loss: 990.061965\n",
      "Epoch 78, loss: 697.273220\n",
      "Epoch 79, loss: 947.206754\n",
      "Epoch 80, loss: 888.554416\n",
      "Epoch 81, loss: 851.291528\n",
      "Epoch 82, loss: 1140.714769\n",
      "Epoch 83, loss: 889.104430\n",
      "Epoch 84, loss: 932.077691\n",
      "Epoch 85, loss: 781.986739\n",
      "Epoch 86, loss: 816.729916\n",
      "Epoch 87, loss: 706.598807\n",
      "Epoch 88, loss: 856.661582\n",
      "Epoch 89, loss: 917.028655\n",
      "Epoch 90, loss: 718.086961\n",
      "Epoch 91, loss: 993.713213\n",
      "Epoch 92, loss: 935.440485\n",
      "Epoch 93, loss: 910.426784\n",
      "Epoch 94, loss: 874.258410\n",
      "Epoch 95, loss: 967.428192\n",
      "Epoch 96, loss: 859.438652\n",
      "Epoch 97, loss: 995.440238\n",
      "Epoch 98, loss: 824.099228\n",
      "Epoch 99, loss: 807.615104\n",
      "Accuracy after training for 100 epochs:  0.146\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 768.906505\n",
      "Epoch 1, loss: 954.563411\n",
      "Epoch 2, loss: 763.110357\n",
      "Epoch 3, loss: 1002.956426\n",
      "Epoch 4, loss: 800.698880\n",
      "Epoch 5, loss: 809.994932\n",
      "Epoch 6, loss: 750.172779\n",
      "Epoch 7, loss: 973.093489\n",
      "Epoch 8, loss: 987.105045\n",
      "Epoch 9, loss: 830.856709\n",
      "Epoch 10, loss: 721.854666\n",
      "Epoch 11, loss: 842.277444\n",
      "Epoch 12, loss: 824.164629\n",
      "Epoch 13, loss: 709.523555\n",
      "Epoch 14, loss: 745.123840\n",
      "Epoch 15, loss: 710.812403\n",
      "Epoch 16, loss: 880.171574\n",
      "Epoch 17, loss: 764.996568\n",
      "Epoch 18, loss: 789.645923\n",
      "Epoch 19, loss: 740.399225\n",
      "Epoch 20, loss: 992.582624\n",
      "Epoch 21, loss: 866.355663\n",
      "Epoch 22, loss: 820.975827\n",
      "Epoch 23, loss: 859.378077\n",
      "Epoch 24, loss: 704.085663\n",
      "Epoch 25, loss: 826.750966\n",
      "Epoch 26, loss: 786.602227\n",
      "Epoch 27, loss: 843.714436\n",
      "Epoch 28, loss: 1032.315734\n",
      "Epoch 29, loss: 922.716496\n",
      "Epoch 30, loss: 823.422783\n",
      "Epoch 31, loss: 912.074642\n",
      "Epoch 32, loss: 1070.285407\n",
      "Epoch 33, loss: 726.430931\n",
      "Epoch 34, loss: 884.843522\n",
      "Epoch 35, loss: 838.977818\n",
      "Epoch 36, loss: 685.170690\n",
      "Epoch 37, loss: 794.584039\n",
      "Epoch 38, loss: 749.829620\n",
      "Epoch 39, loss: 689.155313\n",
      "Epoch 40, loss: 937.453183\n",
      "Epoch 41, loss: 692.468322\n",
      "Epoch 42, loss: 708.556493\n",
      "Epoch 43, loss: 893.029551\n",
      "Epoch 44, loss: 896.059753\n",
      "Epoch 45, loss: 850.744847\n",
      "Epoch 46, loss: 671.284709\n",
      "Epoch 47, loss: 793.244925\n",
      "Epoch 48, loss: 692.449651\n",
      "Epoch 49, loss: 804.712373\n",
      "Epoch 50, loss: 840.350170\n",
      "Epoch 51, loss: 883.597084\n",
      "Epoch 52, loss: 782.063447\n",
      "Epoch 53, loss: 802.828516\n",
      "Epoch 54, loss: 696.102058\n",
      "Epoch 55, loss: 791.478376\n",
      "Epoch 56, loss: 730.973911\n",
      "Epoch 57, loss: 807.347862\n",
      "Epoch 58, loss: 758.391623\n",
      "Epoch 59, loss: 825.267179\n",
      "Epoch 60, loss: 815.884876\n",
      "Epoch 61, loss: 646.327655\n",
      "Epoch 62, loss: 769.600541\n",
      "Epoch 63, loss: 706.428199\n",
      "Epoch 64, loss: 777.263055\n",
      "Epoch 65, loss: 782.961678\n",
      "Epoch 66, loss: 696.302215\n",
      "Epoch 67, loss: 708.542734\n",
      "Epoch 68, loss: 774.360491\n",
      "Epoch 69, loss: 741.635033\n",
      "Epoch 70, loss: 871.295858\n",
      "Epoch 71, loss: 701.903788\n",
      "Epoch 72, loss: 634.623290\n",
      "Epoch 73, loss: 835.275444\n",
      "Epoch 74, loss: 796.680017\n",
      "Epoch 75, loss: 748.047372\n",
      "Epoch 76, loss: 763.819369\n",
      "Epoch 77, loss: 767.046902\n",
      "Epoch 78, loss: 846.534020\n",
      "Epoch 79, loss: 695.487360\n",
      "Epoch 80, loss: 794.818123\n",
      "Epoch 81, loss: 685.319210\n",
      "Epoch 82, loss: 680.958670\n",
      "Epoch 83, loss: 684.699239\n",
      "Epoch 84, loss: 873.077029\n",
      "Epoch 85, loss: 939.715954\n",
      "Epoch 86, loss: 696.148560\n",
      "Epoch 87, loss: 885.227846\n",
      "Epoch 88, loss: 843.848381\n",
      "Epoch 89, loss: 671.876537\n",
      "Epoch 90, loss: 732.156136\n",
      "Epoch 91, loss: 772.232234\n",
      "Epoch 92, loss: 801.077871\n",
      "Epoch 93, loss: 737.272986\n",
      "Epoch 94, loss: 737.499193\n",
      "Epoch 95, loss: 809.690069\n",
      "Epoch 96, loss: 720.699550\n",
      "Epoch 97, loss: 807.748497\n",
      "Epoch 98, loss: 795.590122\n",
      "Epoch 99, loss: 755.554701\n",
      "Epoch 100, loss: 691.144773\n",
      "Epoch 101, loss: 671.038566\n",
      "Epoch 102, loss: 679.986103\n",
      "Epoch 103, loss: 947.741167\n",
      "Epoch 104, loss: 695.824396\n",
      "Epoch 105, loss: 705.680682\n",
      "Epoch 106, loss: 813.651781\n",
      "Epoch 107, loss: 748.685710\n",
      "Epoch 108, loss: 761.441850\n",
      "Epoch 109, loss: 787.395011\n",
      "Epoch 110, loss: 763.040836\n",
      "Epoch 111, loss: 733.298591\n",
      "Epoch 112, loss: 754.160008\n",
      "Epoch 113, loss: 824.017086\n",
      "Epoch 114, loss: 617.652924\n",
      "Epoch 115, loss: 973.243111\n",
      "Epoch 116, loss: 895.619868\n",
      "Epoch 117, loss: 830.117213\n",
      "Epoch 118, loss: 745.540481\n",
      "Epoch 119, loss: 791.885344\n",
      "Epoch 120, loss: 792.748125\n",
      "Epoch 121, loss: 713.427522\n",
      "Epoch 122, loss: 793.173031\n",
      "Epoch 123, loss: 645.304396\n",
      "Epoch 124, loss: 649.393331\n",
      "Epoch 125, loss: 764.269249\n",
      "Epoch 126, loss: 872.432081\n",
      "Epoch 127, loss: 821.409689\n",
      "Epoch 128, loss: 709.500895\n",
      "Epoch 129, loss: 880.556754\n",
      "Epoch 130, loss: 831.558067\n",
      "Epoch 131, loss: 760.973484\n",
      "Epoch 132, loss: 864.896164\n",
      "Epoch 133, loss: 733.570065\n",
      "Epoch 134, loss: 912.092291\n",
      "Epoch 135, loss: 819.782573\n",
      "Epoch 136, loss: 938.815110\n",
      "Epoch 137, loss: 705.020137\n",
      "Epoch 138, loss: 713.213898\n",
      "Epoch 139, loss: 821.975664\n",
      "Epoch 140, loss: 798.694972\n",
      "Epoch 141, loss: 762.917455\n",
      "Epoch 142, loss: 748.347583\n",
      "Epoch 143, loss: 786.527666\n",
      "Epoch 144, loss: 711.980178\n",
      "Epoch 145, loss: 802.579958\n",
      "Epoch 146, loss: 642.461336\n",
      "Epoch 147, loss: 740.566434\n",
      "Epoch 148, loss: 648.022522\n",
      "Epoch 149, loss: 674.058616\n",
      "Epoch 150, loss: 616.664865\n",
      "Epoch 151, loss: 930.783706\n",
      "Epoch 152, loss: 788.537798\n",
      "Epoch 153, loss: 935.406472\n",
      "Epoch 154, loss: 689.915059\n",
      "Epoch 155, loss: 734.474333\n",
      "Epoch 156, loss: 784.253229\n",
      "Epoch 157, loss: 794.302826\n",
      "Epoch 158, loss: 690.701207\n",
      "Epoch 159, loss: 852.238496\n",
      "Epoch 160, loss: 670.272314\n",
      "Epoch 161, loss: 678.187105\n",
      "Epoch 162, loss: 778.106581\n",
      "Epoch 163, loss: 679.020968\n",
      "Epoch 164, loss: 721.143244\n",
      "Epoch 165, loss: 637.819837\n",
      "Epoch 166, loss: 724.070011\n",
      "Epoch 167, loss: 708.011232\n",
      "Epoch 168, loss: 679.361784\n",
      "Epoch 169, loss: 872.218484\n",
      "Epoch 170, loss: 787.797787\n",
      "Epoch 171, loss: 678.580309\n",
      "Epoch 172, loss: 705.635880\n",
      "Epoch 173, loss: 703.091694\n",
      "Epoch 174, loss: 733.931360\n",
      "Epoch 175, loss: 748.419178\n",
      "Epoch 176, loss: 704.568565\n",
      "Epoch 177, loss: 833.439436\n",
      "Epoch 178, loss: 775.676753\n",
      "Epoch 179, loss: 679.492036\n",
      "Epoch 180, loss: 725.278972\n",
      "Epoch 181, loss: 717.737348\n",
      "Epoch 182, loss: 651.327865\n",
      "Epoch 183, loss: 741.350338\n",
      "Epoch 184, loss: 685.907212\n",
      "Epoch 185, loss: 637.994502\n",
      "Epoch 186, loss: 790.469137\n",
      "Epoch 187, loss: 729.320126\n",
      "Epoch 188, loss: 788.298014\n",
      "Epoch 189, loss: 730.997568\n",
      "Epoch 190, loss: 612.594869\n",
      "Epoch 191, loss: 633.543087\n",
      "Epoch 192, loss: 642.081615\n",
      "Epoch 193, loss: 698.160428\n",
      "Epoch 194, loss: 885.335192\n",
      "Epoch 195, loss: 774.703252\n",
      "Epoch 196, loss: 617.688179\n",
      "Epoch 197, loss: 784.109111\n",
      "Epoch 198, loss: 600.678468\n",
      "Epoch 199, loss: 666.198402\n",
      "Epoch 0, loss: 752.505788\n",
      "Epoch 1, loss: 941.619894\n",
      "Epoch 2, loss: 728.523268\n",
      "Epoch 3, loss: 837.269398\n",
      "Epoch 4, loss: 948.147419\n",
      "Epoch 5, loss: 767.457502\n",
      "Epoch 6, loss: 1003.056896\n",
      "Epoch 7, loss: 736.036204\n",
      "Epoch 8, loss: 1006.131216\n",
      "Epoch 9, loss: 907.849165\n",
      "Epoch 10, loss: 742.551549\n",
      "Epoch 11, loss: 993.298059\n",
      "Epoch 12, loss: 854.192690\n",
      "Epoch 13, loss: 867.052785\n",
      "Epoch 14, loss: 844.727910\n",
      "Epoch 15, loss: 1053.935027\n",
      "Epoch 16, loss: 857.578472\n",
      "Epoch 17, loss: 755.863075\n",
      "Epoch 18, loss: 785.272118\n",
      "Epoch 19, loss: 674.565571\n",
      "Epoch 20, loss: 887.861706\n",
      "Epoch 21, loss: 864.565138\n",
      "Epoch 22, loss: 727.074660\n",
      "Epoch 23, loss: 1130.174634\n",
      "Epoch 24, loss: 835.884815\n",
      "Epoch 25, loss: 863.396159\n",
      "Epoch 26, loss: 974.375304\n",
      "Epoch 27, loss: 716.886633\n",
      "Epoch 28, loss: 773.820952\n",
      "Epoch 29, loss: 751.071835\n",
      "Epoch 30, loss: 724.139975\n",
      "Epoch 31, loss: 720.421633\n",
      "Epoch 32, loss: 779.614165\n",
      "Epoch 33, loss: 752.230293\n",
      "Epoch 34, loss: 854.600053\n",
      "Epoch 35, loss: 897.470100\n",
      "Epoch 36, loss: 862.712785\n",
      "Epoch 37, loss: 788.704538\n",
      "Epoch 38, loss: 749.766395\n",
      "Epoch 39, loss: 965.282556\n",
      "Epoch 40, loss: 861.218928\n",
      "Epoch 41, loss: 763.279757\n",
      "Epoch 42, loss: 844.369308\n",
      "Epoch 43, loss: 920.089709\n",
      "Epoch 44, loss: 742.297886\n",
      "Epoch 45, loss: 703.412547\n",
      "Epoch 46, loss: 805.037568\n",
      "Epoch 47, loss: 747.194532\n",
      "Epoch 48, loss: 899.855126\n",
      "Epoch 49, loss: 870.350011\n",
      "Epoch 50, loss: 845.719928\n",
      "Epoch 51, loss: 708.788735\n",
      "Epoch 52, loss: 686.229228\n",
      "Epoch 53, loss: 733.101538\n",
      "Epoch 54, loss: 698.741152\n",
      "Epoch 55, loss: 867.288283\n",
      "Epoch 56, loss: 866.404826\n",
      "Epoch 57, loss: 765.354916\n",
      "Epoch 58, loss: 843.131752\n",
      "Epoch 59, loss: 833.655268\n",
      "Epoch 60, loss: 946.109892\n",
      "Epoch 61, loss: 872.186459\n",
      "Epoch 62, loss: 798.577939\n",
      "Epoch 63, loss: 712.691783\n",
      "Epoch 64, loss: 941.697203\n",
      "Epoch 65, loss: 776.204193\n",
      "Epoch 66, loss: 800.889089\n",
      "Epoch 67, loss: 859.536115\n",
      "Epoch 68, loss: 754.156599\n",
      "Epoch 69, loss: 707.211813\n",
      "Epoch 70, loss: 719.057523\n",
      "Epoch 71, loss: 670.735113\n",
      "Epoch 72, loss: 836.561819\n",
      "Epoch 73, loss: 659.584928\n",
      "Epoch 74, loss: 691.418130\n",
      "Epoch 75, loss: 723.509647\n",
      "Epoch 76, loss: 834.895110\n",
      "Epoch 77, loss: 796.075351\n",
      "Epoch 78, loss: 690.436864\n",
      "Epoch 79, loss: 660.388925\n",
      "Epoch 80, loss: 780.228243\n",
      "Epoch 81, loss: 801.224557\n",
      "Epoch 82, loss: 810.657835\n",
      "Epoch 83, loss: 736.553134\n",
      "Epoch 84, loss: 693.193070\n",
      "Epoch 85, loss: 770.377595\n",
      "Epoch 86, loss: 749.036270\n",
      "Epoch 87, loss: 681.892732\n",
      "Epoch 88, loss: 784.286354\n",
      "Epoch 89, loss: 686.456494\n",
      "Epoch 90, loss: 759.314160\n",
      "Epoch 91, loss: 895.120034\n",
      "Epoch 92, loss: 749.024102\n",
      "Epoch 93, loss: 928.109174\n",
      "Epoch 94, loss: 665.697626\n",
      "Epoch 95, loss: 668.902127\n",
      "Epoch 96, loss: 904.748633\n",
      "Epoch 97, loss: 693.025328\n",
      "Epoch 98, loss: 916.919973\n",
      "Epoch 99, loss: 653.640644\n",
      "Epoch 100, loss: 804.533836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, loss: 753.087070\n",
      "Epoch 102, loss: 781.804239\n",
      "Epoch 103, loss: 703.357949\n",
      "Epoch 104, loss: 660.613343\n",
      "Epoch 105, loss: 791.255536\n",
      "Epoch 106, loss: 780.111248\n",
      "Epoch 107, loss: 712.317217\n",
      "Epoch 108, loss: 805.446407\n",
      "Epoch 109, loss: 750.942991\n",
      "Epoch 110, loss: 872.719314\n",
      "Epoch 111, loss: 657.541591\n",
      "Epoch 112, loss: 658.245073\n",
      "Epoch 113, loss: 766.145029\n",
      "Epoch 114, loss: 857.111747\n",
      "Epoch 115, loss: 730.502999\n",
      "Epoch 116, loss: 799.024741\n",
      "Epoch 117, loss: 819.757243\n",
      "Epoch 118, loss: 643.693487\n",
      "Epoch 119, loss: 751.540821\n",
      "Epoch 120, loss: 734.727880\n",
      "Epoch 121, loss: 652.618418\n",
      "Epoch 122, loss: 906.631054\n",
      "Epoch 123, loss: 879.061715\n",
      "Epoch 124, loss: 722.288506\n",
      "Epoch 125, loss: 743.453319\n",
      "Epoch 126, loss: 665.377053\n",
      "Epoch 127, loss: 686.805969\n",
      "Epoch 128, loss: 776.042855\n",
      "Epoch 129, loss: 882.510534\n",
      "Epoch 130, loss: 772.290162\n",
      "Epoch 131, loss: 699.329074\n",
      "Epoch 132, loss: 656.047847\n",
      "Epoch 133, loss: 746.495625\n",
      "Epoch 134, loss: 659.484019\n",
      "Epoch 135, loss: 635.950916\n",
      "Epoch 136, loss: 721.082650\n",
      "Epoch 137, loss: 754.464705\n",
      "Epoch 138, loss: 687.783285\n",
      "Epoch 139, loss: 841.207216\n",
      "Epoch 140, loss: 679.087526\n",
      "Epoch 141, loss: 690.252141\n",
      "Epoch 142, loss: 783.425901\n",
      "Epoch 143, loss: 803.817151\n",
      "Epoch 144, loss: 928.157914\n",
      "Epoch 145, loss: 772.622220\n",
      "Epoch 146, loss: 802.521469\n",
      "Epoch 147, loss: 786.590129\n",
      "Epoch 148, loss: 766.026750\n",
      "Epoch 149, loss: 653.174779\n",
      "Epoch 150, loss: 665.514138\n",
      "Epoch 151, loss: 686.373018\n",
      "Epoch 152, loss: 638.440291\n",
      "Epoch 153, loss: 732.983196\n",
      "Epoch 154, loss: 749.056808\n",
      "Epoch 155, loss: 926.408719\n",
      "Epoch 156, loss: 759.122766\n",
      "Epoch 157, loss: 702.111072\n",
      "Epoch 158, loss: 850.309740\n",
      "Epoch 159, loss: 706.054654\n",
      "Epoch 160, loss: 705.724857\n",
      "Epoch 161, loss: 699.077635\n",
      "Epoch 162, loss: 684.613782\n",
      "Epoch 163, loss: 716.967292\n",
      "Epoch 164, loss: 658.565871\n",
      "Epoch 165, loss: 625.304680\n",
      "Epoch 166, loss: 800.797630\n",
      "Epoch 167, loss: 641.186461\n",
      "Epoch 168, loss: 826.607285\n",
      "Epoch 169, loss: 723.761143\n",
      "Epoch 170, loss: 694.256938\n",
      "Epoch 171, loss: 769.823049\n",
      "Epoch 172, loss: 879.979811\n",
      "Epoch 173, loss: 734.688788\n",
      "Epoch 174, loss: 709.782952\n",
      "Epoch 175, loss: 757.207539\n",
      "Epoch 176, loss: 665.701954\n",
      "Epoch 177, loss: 812.328120\n",
      "Epoch 178, loss: 880.407031\n",
      "Epoch 179, loss: 749.517129\n",
      "Epoch 180, loss: 807.487478\n",
      "Epoch 181, loss: 643.819858\n",
      "Epoch 182, loss: 860.276422\n",
      "Epoch 183, loss: 843.873023\n",
      "Epoch 184, loss: 703.977003\n",
      "Epoch 185, loss: 837.514705\n",
      "Epoch 186, loss: 613.408270\n",
      "Epoch 187, loss: 927.295916\n",
      "Epoch 188, loss: 809.041640\n",
      "Epoch 189, loss: 631.362262\n",
      "Epoch 190, loss: 675.940981\n",
      "Epoch 191, loss: 780.339055\n",
      "Epoch 192, loss: 757.265585\n",
      "Epoch 193, loss: 857.432816\n",
      "Epoch 194, loss: 965.315523\n",
      "Epoch 195, loss: 683.587006\n",
      "Epoch 196, loss: 698.950535\n",
      "Epoch 197, loss: 616.113253\n",
      "Epoch 198, loss: 652.139680\n",
      "Epoch 199, loss: 851.550314\n",
      "Epoch 0, loss: 837.063314\n",
      "Epoch 1, loss: 813.912347\n",
      "Epoch 2, loss: 823.359729\n",
      "Epoch 3, loss: 827.471134\n",
      "Epoch 4, loss: 984.345174\n",
      "Epoch 5, loss: 863.843672\n",
      "Epoch 6, loss: 816.773339\n",
      "Epoch 7, loss: 991.485015\n",
      "Epoch 8, loss: 789.171432\n",
      "Epoch 9, loss: 1085.342367\n",
      "Epoch 10, loss: 885.095043\n",
      "Epoch 11, loss: 835.595355\n",
      "Epoch 12, loss: 743.160196\n",
      "Epoch 13, loss: 748.030214\n",
      "Epoch 14, loss: 715.265305\n",
      "Epoch 15, loss: 786.487319\n",
      "Epoch 16, loss: 827.246196\n",
      "Epoch 17, loss: 770.135882\n",
      "Epoch 18, loss: 721.750605\n",
      "Epoch 19, loss: 795.061409\n",
      "Epoch 20, loss: 799.364847\n",
      "Epoch 21, loss: 845.004354\n",
      "Epoch 22, loss: 845.118102\n",
      "Epoch 23, loss: 952.248326\n",
      "Epoch 24, loss: 793.899649\n",
      "Epoch 25, loss: 843.344464\n",
      "Epoch 26, loss: 710.552328\n",
      "Epoch 27, loss: 903.026880\n",
      "Epoch 28, loss: 889.036722\n",
      "Epoch 29, loss: 713.978126\n",
      "Epoch 30, loss: 805.975657\n",
      "Epoch 31, loss: 838.840683\n",
      "Epoch 32, loss: 801.179956\n",
      "Epoch 33, loss: 1014.428066\n",
      "Epoch 34, loss: 1033.564463\n",
      "Epoch 35, loss: 719.845277\n",
      "Epoch 36, loss: 820.298088\n",
      "Epoch 37, loss: 753.496906\n",
      "Epoch 38, loss: 781.604553\n",
      "Epoch 39, loss: 813.243409\n",
      "Epoch 40, loss: 695.611573\n",
      "Epoch 41, loss: 723.258333\n",
      "Epoch 42, loss: 906.511462\n",
      "Epoch 43, loss: 808.699914\n",
      "Epoch 44, loss: 706.835375\n",
      "Epoch 45, loss: 1005.014588\n",
      "Epoch 46, loss: 736.822730\n",
      "Epoch 47, loss: 850.566733\n",
      "Epoch 48, loss: 791.939155\n",
      "Epoch 49, loss: 706.440075\n",
      "Epoch 50, loss: 876.492250\n",
      "Epoch 51, loss: 702.009113\n",
      "Epoch 52, loss: 846.724456\n",
      "Epoch 53, loss: 826.163591\n",
      "Epoch 54, loss: 1078.748425\n",
      "Epoch 55, loss: 869.994936\n",
      "Epoch 56, loss: 790.912993\n",
      "Epoch 57, loss: 911.600867\n",
      "Epoch 58, loss: 684.020068\n",
      "Epoch 59, loss: 860.571305\n",
      "Epoch 60, loss: 729.378538\n",
      "Epoch 61, loss: 731.039803\n",
      "Epoch 62, loss: 780.699178\n",
      "Epoch 63, loss: 780.042499\n",
      "Epoch 64, loss: 861.534727\n",
      "Epoch 65, loss: 699.919910\n",
      "Epoch 66, loss: 843.603292\n",
      "Epoch 67, loss: 709.149203\n",
      "Epoch 68, loss: 881.806930\n",
      "Epoch 69, loss: 679.329455\n",
      "Epoch 70, loss: 802.021610\n",
      "Epoch 71, loss: 823.650545\n",
      "Epoch 72, loss: 721.854143\n",
      "Epoch 73, loss: 937.627419\n",
      "Epoch 74, loss: 679.166759\n",
      "Epoch 75, loss: 841.129402\n",
      "Epoch 76, loss: 739.731450\n",
      "Epoch 77, loss: 776.083362\n",
      "Epoch 78, loss: 692.376426\n",
      "Epoch 79, loss: 752.788212\n",
      "Epoch 80, loss: 768.147771\n",
      "Epoch 81, loss: 813.430957\n",
      "Epoch 82, loss: 664.152639\n",
      "Epoch 83, loss: 740.707708\n",
      "Epoch 84, loss: 683.411650\n",
      "Epoch 85, loss: 742.456718\n",
      "Epoch 86, loss: 780.534198\n",
      "Epoch 87, loss: 687.713832\n",
      "Epoch 88, loss: 818.601943\n",
      "Epoch 89, loss: 678.507177\n",
      "Epoch 90, loss: 703.827038\n",
      "Epoch 91, loss: 779.855200\n",
      "Epoch 92, loss: 811.907571\n",
      "Epoch 93, loss: 616.143266\n",
      "Epoch 94, loss: 824.232774\n",
      "Epoch 95, loss: 678.190915\n",
      "Epoch 96, loss: 667.464959\n",
      "Epoch 97, loss: 805.656381\n",
      "Epoch 98, loss: 715.703362\n",
      "Epoch 99, loss: 604.487086\n",
      "Epoch 100, loss: 729.342238\n",
      "Epoch 101, loss: 799.525351\n",
      "Epoch 102, loss: 878.981014\n",
      "Epoch 103, loss: 675.364777\n",
      "Epoch 104, loss: 732.151773\n",
      "Epoch 105, loss: 872.438722\n",
      "Epoch 106, loss: 723.555141\n",
      "Epoch 107, loss: 670.636272\n",
      "Epoch 108, loss: 840.548531\n",
      "Epoch 109, loss: 721.125568\n",
      "Epoch 110, loss: 769.817492\n",
      "Epoch 111, loss: 724.503462\n",
      "Epoch 112, loss: 647.511322\n",
      "Epoch 113, loss: 786.406274\n",
      "Epoch 114, loss: 758.477244\n",
      "Epoch 115, loss: 832.633361\n",
      "Epoch 116, loss: 638.936133\n",
      "Epoch 117, loss: 665.570533\n",
      "Epoch 118, loss: 711.179533\n",
      "Epoch 119, loss: 830.374081\n",
      "Epoch 120, loss: 709.609185\n",
      "Epoch 121, loss: 848.050795\n",
      "Epoch 122, loss: 793.466688\n",
      "Epoch 123, loss: 769.628252\n",
      "Epoch 124, loss: 788.072397\n",
      "Epoch 125, loss: 740.521086\n",
      "Epoch 126, loss: 677.369160\n",
      "Epoch 127, loss: 838.029208\n",
      "Epoch 128, loss: 888.095784\n",
      "Epoch 129, loss: 738.613843\n",
      "Epoch 130, loss: 674.032824\n",
      "Epoch 131, loss: 826.578376\n",
      "Epoch 132, loss: 645.181994\n",
      "Epoch 133, loss: 762.686915\n",
      "Epoch 134, loss: 623.041641\n",
      "Epoch 135, loss: 773.374375\n",
      "Epoch 136, loss: 747.628953\n",
      "Epoch 137, loss: 730.362066\n",
      "Epoch 138, loss: 826.085904\n",
      "Epoch 139, loss: 826.968541\n",
      "Epoch 140, loss: 743.846864\n",
      "Epoch 141, loss: 746.399098\n",
      "Epoch 142, loss: 716.530717\n",
      "Epoch 143, loss: 661.103396\n",
      "Epoch 144, loss: 689.029221\n",
      "Epoch 145, loss: 814.826572\n",
      "Epoch 146, loss: 712.039638\n",
      "Epoch 147, loss: 649.557303\n",
      "Epoch 148, loss: 765.656590\n",
      "Epoch 149, loss: 659.554746\n",
      "Epoch 150, loss: 717.288297\n",
      "Epoch 151, loss: 659.424195\n",
      "Epoch 152, loss: 667.633344\n",
      "Epoch 153, loss: 750.371648\n",
      "Epoch 154, loss: 674.172387\n",
      "Epoch 155, loss: 795.377745\n",
      "Epoch 156, loss: 738.266911\n",
      "Epoch 157, loss: 851.653419\n",
      "Epoch 158, loss: 604.530006\n",
      "Epoch 159, loss: 861.244079\n",
      "Epoch 160, loss: 930.058109\n",
      "Epoch 161, loss: 673.693330\n",
      "Epoch 162, loss: 885.671172\n",
      "Epoch 163, loss: 763.194815\n",
      "Epoch 164, loss: 749.935056\n",
      "Epoch 165, loss: 911.839913\n",
      "Epoch 166, loss: 675.288174\n",
      "Epoch 167, loss: 746.226639\n",
      "Epoch 168, loss: 728.225884\n",
      "Epoch 169, loss: 846.126279\n",
      "Epoch 170, loss: 704.640509\n",
      "Epoch 171, loss: 701.818885\n",
      "Epoch 172, loss: 684.420172\n",
      "Epoch 173, loss: 761.038287\n",
      "Epoch 174, loss: 699.846793\n",
      "Epoch 175, loss: 688.481199\n",
      "Epoch 176, loss: 777.426916\n",
      "Epoch 177, loss: 672.055188\n",
      "Epoch 178, loss: 919.860329\n",
      "Epoch 179, loss: 758.635371\n",
      "Epoch 180, loss: 755.054558\n",
      "Epoch 181, loss: 674.667007\n",
      "Epoch 182, loss: 788.059254\n",
      "Epoch 183, loss: 633.952270\n",
      "Epoch 184, loss: 803.989700\n",
      "Epoch 185, loss: 699.147735\n",
      "Epoch 186, loss: 668.546317\n",
      "Epoch 187, loss: 725.539716\n",
      "Epoch 188, loss: 692.802077\n",
      "Epoch 189, loss: 776.671192\n",
      "Epoch 190, loss: 746.582566\n",
      "Epoch 191, loss: 856.090560\n",
      "Epoch 192, loss: 859.475201\n",
      "Epoch 193, loss: 853.453608\n",
      "Epoch 194, loss: 709.385643\n",
      "Epoch 195, loss: 768.873451\n",
      "Epoch 196, loss: 745.500904\n",
      "Epoch 197, loss: 604.215889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, loss: 716.686606\n",
      "Epoch 199, loss: 685.118531\n",
      "Epoch 0, loss: 686.253848\n",
      "Epoch 1, loss: 680.222149\n",
      "Epoch 2, loss: 675.740729\n",
      "Epoch 3, loss: 673.459591\n",
      "Epoch 4, loss: 669.648610\n",
      "Epoch 5, loss: 663.484288\n",
      "Epoch 6, loss: 671.467003\n",
      "Epoch 7, loss: 664.394169\n",
      "Epoch 8, loss: 653.554259\n",
      "Epoch 9, loss: 655.479216\n",
      "Epoch 10, loss: 647.894033\n",
      "Epoch 11, loss: 647.759472\n",
      "Epoch 12, loss: 650.846110\n",
      "Epoch 13, loss: 643.718464\n",
      "Epoch 14, loss: 642.208624\n",
      "Epoch 15, loss: 654.346629\n",
      "Epoch 16, loss: 647.964524\n",
      "Epoch 17, loss: 636.542911\n",
      "Epoch 18, loss: 641.393752\n",
      "Epoch 19, loss: 644.301866\n",
      "Epoch 20, loss: 643.074916\n",
      "Epoch 21, loss: 647.692286\n",
      "Epoch 22, loss: 656.884131\n",
      "Epoch 23, loss: 643.095989\n",
      "Epoch 24, loss: 648.676545\n",
      "Epoch 25, loss: 638.879637\n",
      "Epoch 26, loss: 632.742371\n",
      "Epoch 27, loss: 645.716522\n",
      "Epoch 28, loss: 644.967282\n",
      "Epoch 29, loss: 639.364280\n",
      "Epoch 30, loss: 622.740475\n",
      "Epoch 31, loss: 649.600791\n",
      "Epoch 32, loss: 645.839981\n",
      "Epoch 33, loss: 632.332536\n",
      "Epoch 34, loss: 639.327714\n",
      "Epoch 35, loss: 656.298082\n",
      "Epoch 36, loss: 655.102765\n",
      "Epoch 37, loss: 632.079211\n",
      "Epoch 38, loss: 646.088689\n",
      "Epoch 39, loss: 634.361920\n",
      "Epoch 40, loss: 644.229407\n",
      "Epoch 41, loss: 630.786360\n",
      "Epoch 42, loss: 642.019262\n",
      "Epoch 43, loss: 628.542882\n",
      "Epoch 44, loss: 639.314861\n",
      "Epoch 45, loss: 643.462036\n",
      "Epoch 46, loss: 634.985895\n",
      "Epoch 47, loss: 635.011489\n",
      "Epoch 48, loss: 641.935600\n",
      "Epoch 49, loss: 637.327389\n",
      "Epoch 50, loss: 631.929690\n",
      "Epoch 51, loss: 640.251334\n",
      "Epoch 52, loss: 636.915221\n",
      "Epoch 53, loss: 634.287018\n",
      "Epoch 54, loss: 631.975681\n",
      "Epoch 55, loss: 641.725598\n",
      "Epoch 56, loss: 634.710050\n",
      "Epoch 57, loss: 617.073078\n",
      "Epoch 58, loss: 637.838004\n",
      "Epoch 59, loss: 636.112103\n",
      "Epoch 60, loss: 644.499697\n",
      "Epoch 61, loss: 638.088359\n",
      "Epoch 62, loss: 635.443896\n",
      "Epoch 63, loss: 620.299782\n",
      "Epoch 64, loss: 618.239928\n",
      "Epoch 65, loss: 612.793757\n",
      "Epoch 66, loss: 640.475750\n",
      "Epoch 67, loss: 625.973921\n",
      "Epoch 68, loss: 640.434631\n",
      "Epoch 69, loss: 622.986017\n",
      "Epoch 70, loss: 639.728380\n",
      "Epoch 71, loss: 622.101486\n",
      "Epoch 72, loss: 638.027778\n",
      "Epoch 73, loss: 641.222664\n",
      "Epoch 74, loss: 630.278445\n",
      "Epoch 75, loss: 637.698548\n",
      "Epoch 76, loss: 635.890768\n",
      "Epoch 77, loss: 631.025251\n",
      "Epoch 78, loss: 625.231568\n",
      "Epoch 79, loss: 620.386337\n",
      "Epoch 80, loss: 628.789522\n",
      "Epoch 81, loss: 616.141248\n",
      "Epoch 82, loss: 608.337803\n",
      "Epoch 83, loss: 628.131722\n",
      "Epoch 84, loss: 633.189900\n",
      "Epoch 85, loss: 628.286842\n",
      "Epoch 86, loss: 619.582816\n",
      "Epoch 87, loss: 638.253834\n",
      "Epoch 88, loss: 639.121104\n",
      "Epoch 89, loss: 628.032277\n",
      "Epoch 90, loss: 631.483385\n",
      "Epoch 91, loss: 624.909082\n",
      "Epoch 92, loss: 638.529061\n",
      "Epoch 93, loss: 619.949594\n",
      "Epoch 94, loss: 631.578861\n",
      "Epoch 95, loss: 639.732978\n",
      "Epoch 96, loss: 598.231085\n",
      "Epoch 97, loss: 628.513761\n",
      "Epoch 98, loss: 619.641812\n",
      "Epoch 99, loss: 620.557724\n",
      "Epoch 100, loss: 645.117394\n",
      "Epoch 101, loss: 594.871275\n",
      "Epoch 102, loss: 622.832451\n",
      "Epoch 103, loss: 638.082287\n",
      "Epoch 104, loss: 616.658545\n",
      "Epoch 105, loss: 655.150482\n",
      "Epoch 106, loss: 621.668473\n",
      "Epoch 107, loss: 619.134757\n",
      "Epoch 108, loss: 630.653774\n",
      "Epoch 109, loss: 611.329291\n",
      "Epoch 110, loss: 619.170728\n",
      "Epoch 111, loss: 629.842985\n",
      "Epoch 112, loss: 618.030357\n",
      "Epoch 113, loss: 631.498428\n",
      "Epoch 114, loss: 603.856378\n",
      "Epoch 115, loss: 618.363776\n",
      "Epoch 116, loss: 621.061064\n",
      "Epoch 117, loss: 626.605945\n",
      "Epoch 118, loss: 635.780978\n",
      "Epoch 119, loss: 618.471205\n",
      "Epoch 120, loss: 626.006111\n",
      "Epoch 121, loss: 623.372448\n",
      "Epoch 122, loss: 608.728629\n",
      "Epoch 123, loss: 624.127899\n",
      "Epoch 124, loss: 638.607382\n",
      "Epoch 125, loss: 631.607002\n",
      "Epoch 126, loss: 605.391405\n",
      "Epoch 127, loss: 626.504673\n",
      "Epoch 128, loss: 604.457385\n",
      "Epoch 129, loss: 629.365562\n",
      "Epoch 130, loss: 635.267337\n",
      "Epoch 131, loss: 625.729074\n",
      "Epoch 132, loss: 622.343889\n",
      "Epoch 133, loss: 629.429968\n",
      "Epoch 134, loss: 611.428744\n",
      "Epoch 135, loss: 626.613324\n",
      "Epoch 136, loss: 639.454423\n",
      "Epoch 137, loss: 599.818547\n",
      "Epoch 138, loss: 620.846896\n",
      "Epoch 139, loss: 618.962556\n",
      "Epoch 140, loss: 624.070151\n",
      "Epoch 141, loss: 623.736729\n",
      "Epoch 142, loss: 625.000536\n",
      "Epoch 143, loss: 622.149719\n",
      "Epoch 144, loss: 647.925457\n",
      "Epoch 145, loss: 628.024345\n",
      "Epoch 146, loss: 640.618462\n",
      "Epoch 147, loss: 611.887430\n",
      "Epoch 148, loss: 614.482195\n",
      "Epoch 149, loss: 622.840320\n",
      "Epoch 150, loss: 615.066409\n",
      "Epoch 151, loss: 623.924150\n",
      "Epoch 152, loss: 623.399772\n",
      "Epoch 153, loss: 626.623328\n",
      "Epoch 154, loss: 631.209376\n",
      "Epoch 155, loss: 619.189664\n",
      "Epoch 156, loss: 620.174448\n",
      "Epoch 157, loss: 629.509154\n",
      "Epoch 158, loss: 591.008875\n",
      "Epoch 159, loss: 625.515760\n",
      "Epoch 160, loss: 616.871032\n",
      "Epoch 161, loss: 622.832843\n",
      "Epoch 162, loss: 613.747497\n",
      "Epoch 163, loss: 626.342987\n",
      "Epoch 164, loss: 617.675578\n",
      "Epoch 165, loss: 613.401079\n",
      "Epoch 166, loss: 618.140799\n",
      "Epoch 167, loss: 618.453833\n",
      "Epoch 168, loss: 634.170059\n",
      "Epoch 169, loss: 616.548946\n",
      "Epoch 170, loss: 610.924274\n",
      "Epoch 171, loss: 604.076654\n",
      "Epoch 172, loss: 633.266990\n",
      "Epoch 173, loss: 602.730233\n",
      "Epoch 174, loss: 625.764456\n",
      "Epoch 175, loss: 621.821402\n",
      "Epoch 176, loss: 610.214964\n",
      "Epoch 177, loss: 617.393995\n",
      "Epoch 178, loss: 620.682471\n",
      "Epoch 179, loss: 605.927883\n",
      "Epoch 180, loss: 620.085549\n",
      "Epoch 181, loss: 623.819468\n",
      "Epoch 182, loss: 612.457972\n",
      "Epoch 183, loss: 593.296811\n",
      "Epoch 184, loss: 615.482635\n",
      "Epoch 185, loss: 619.958138\n",
      "Epoch 186, loss: 619.380117\n",
      "Epoch 187, loss: 624.542048\n",
      "Epoch 188, loss: 617.953292\n",
      "Epoch 189, loss: 609.966215\n",
      "Epoch 190, loss: 618.783520\n",
      "Epoch 191, loss: 618.705698\n",
      "Epoch 192, loss: 603.272726\n",
      "Epoch 193, loss: 608.212858\n",
      "Epoch 194, loss: 608.300213\n",
      "Epoch 195, loss: 635.606578\n",
      "Epoch 196, loss: 657.779043\n",
      "Epoch 197, loss: 619.479739\n",
      "Epoch 198, loss: 607.675181\n",
      "Epoch 199, loss: 619.011321\n",
      "Epoch 0, loss: 685.935926\n",
      "Epoch 1, loss: 680.635126\n",
      "Epoch 2, loss: 673.127385\n",
      "Epoch 3, loss: 677.815349\n",
      "Epoch 4, loss: 669.262001\n",
      "Epoch 5, loss: 666.211313\n",
      "Epoch 6, loss: 665.208443\n",
      "Epoch 7, loss: 659.113727\n",
      "Epoch 8, loss: 664.510408\n",
      "Epoch 9, loss: 650.769457\n",
      "Epoch 10, loss: 658.788904\n",
      "Epoch 11, loss: 661.133555\n",
      "Epoch 12, loss: 652.040909\n",
      "Epoch 13, loss: 646.911321\n",
      "Epoch 14, loss: 656.592288\n",
      "Epoch 15, loss: 648.431367\n",
      "Epoch 16, loss: 650.455390\n",
      "Epoch 17, loss: 658.170399\n",
      "Epoch 18, loss: 647.889304\n",
      "Epoch 19, loss: 639.600538\n",
      "Epoch 20, loss: 639.931906\n",
      "Epoch 21, loss: 636.140648\n",
      "Epoch 22, loss: 649.748248\n",
      "Epoch 23, loss: 643.199629\n",
      "Epoch 24, loss: 626.386181\n",
      "Epoch 25, loss: 638.564797\n",
      "Epoch 26, loss: 647.301617\n",
      "Epoch 27, loss: 656.516962\n",
      "Epoch 28, loss: 638.014238\n",
      "Epoch 29, loss: 638.940576\n",
      "Epoch 30, loss: 630.104093\n",
      "Epoch 31, loss: 637.480023\n",
      "Epoch 32, loss: 651.678605\n",
      "Epoch 33, loss: 631.241891\n",
      "Epoch 34, loss: 652.168645\n",
      "Epoch 35, loss: 621.578132\n",
      "Epoch 36, loss: 646.050877\n",
      "Epoch 37, loss: 631.829667\n",
      "Epoch 38, loss: 628.996773\n",
      "Epoch 39, loss: 637.709930\n",
      "Epoch 40, loss: 630.501735\n",
      "Epoch 41, loss: 618.832981\n",
      "Epoch 42, loss: 627.886253\n",
      "Epoch 43, loss: 632.114738\n",
      "Epoch 44, loss: 638.122103\n",
      "Epoch 45, loss: 632.998757\n",
      "Epoch 46, loss: 654.770242\n",
      "Epoch 47, loss: 629.130479\n",
      "Epoch 48, loss: 654.719445\n",
      "Epoch 49, loss: 640.030666\n",
      "Epoch 50, loss: 637.386934\n",
      "Epoch 51, loss: 639.126586\n",
      "Epoch 52, loss: 636.707025\n",
      "Epoch 53, loss: 630.172918\n",
      "Epoch 54, loss: 624.872712\n",
      "Epoch 55, loss: 633.787796\n",
      "Epoch 56, loss: 615.589893\n",
      "Epoch 57, loss: 638.885516\n",
      "Epoch 58, loss: 647.258135\n",
      "Epoch 59, loss: 633.443342\n",
      "Epoch 60, loss: 611.454371\n",
      "Epoch 61, loss: 634.712928\n",
      "Epoch 62, loss: 636.989557\n",
      "Epoch 63, loss: 635.341326\n",
      "Epoch 64, loss: 629.277147\n",
      "Epoch 65, loss: 644.204520\n",
      "Epoch 66, loss: 626.676548\n",
      "Epoch 67, loss: 621.336495\n",
      "Epoch 68, loss: 615.978492\n",
      "Epoch 69, loss: 632.450418\n",
      "Epoch 70, loss: 627.544303\n",
      "Epoch 71, loss: 642.601067\n",
      "Epoch 72, loss: 630.748833\n",
      "Epoch 73, loss: 651.738599\n",
      "Epoch 74, loss: 628.202426\n",
      "Epoch 75, loss: 642.991599\n",
      "Epoch 76, loss: 638.157808\n",
      "Epoch 77, loss: 623.741822\n",
      "Epoch 78, loss: 627.159012\n",
      "Epoch 79, loss: 626.356668\n",
      "Epoch 80, loss: 628.443476\n",
      "Epoch 81, loss: 634.254290\n",
      "Epoch 82, loss: 627.606283\n",
      "Epoch 83, loss: 642.219793\n",
      "Epoch 84, loss: 638.842588\n",
      "Epoch 85, loss: 652.509353\n",
      "Epoch 86, loss: 619.351678\n",
      "Epoch 87, loss: 627.405791\n",
      "Epoch 88, loss: 623.420305\n",
      "Epoch 89, loss: 615.479543\n",
      "Epoch 90, loss: 645.697482\n",
      "Epoch 91, loss: 623.970842\n",
      "Epoch 92, loss: 637.963209\n",
      "Epoch 93, loss: 607.746732\n",
      "Epoch 94, loss: 617.484873\n",
      "Epoch 95, loss: 647.598285\n",
      "Epoch 96, loss: 641.063910\n",
      "Epoch 97, loss: 637.056613\n",
      "Epoch 98, loss: 630.917185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 618.321148\n",
      "Epoch 100, loss: 625.916113\n",
      "Epoch 101, loss: 616.160981\n",
      "Epoch 102, loss: 613.948897\n",
      "Epoch 103, loss: 625.019686\n",
      "Epoch 104, loss: 643.314041\n",
      "Epoch 105, loss: 609.040716\n",
      "Epoch 106, loss: 627.915701\n",
      "Epoch 107, loss: 636.497485\n",
      "Epoch 108, loss: 625.562682\n",
      "Epoch 109, loss: 612.797913\n",
      "Epoch 110, loss: 626.610781\n",
      "Epoch 111, loss: 627.270061\n",
      "Epoch 112, loss: 630.984930\n",
      "Epoch 113, loss: 623.585463\n",
      "Epoch 114, loss: 621.698360\n",
      "Epoch 115, loss: 633.149653\n",
      "Epoch 116, loss: 610.511901\n",
      "Epoch 117, loss: 617.931143\n",
      "Epoch 118, loss: 629.423014\n",
      "Epoch 119, loss: 628.210604\n",
      "Epoch 120, loss: 636.559032\n",
      "Epoch 121, loss: 628.896830\n",
      "Epoch 122, loss: 606.703066\n",
      "Epoch 123, loss: 617.005932\n",
      "Epoch 124, loss: 619.843325\n",
      "Epoch 125, loss: 633.275522\n",
      "Epoch 126, loss: 623.896357\n",
      "Epoch 127, loss: 623.016229\n",
      "Epoch 128, loss: 627.201453\n",
      "Epoch 129, loss: 613.053873\n",
      "Epoch 130, loss: 609.081117\n",
      "Epoch 131, loss: 621.728859\n",
      "Epoch 132, loss: 626.062201\n",
      "Epoch 133, loss: 621.405017\n",
      "Epoch 134, loss: 635.569843\n",
      "Epoch 135, loss: 631.720023\n",
      "Epoch 136, loss: 622.656677\n",
      "Epoch 137, loss: 622.358666\n",
      "Epoch 138, loss: 618.387451\n",
      "Epoch 139, loss: 622.254239\n",
      "Epoch 140, loss: 625.625653\n",
      "Epoch 141, loss: 635.279444\n",
      "Epoch 142, loss: 610.462277\n",
      "Epoch 143, loss: 628.699270\n",
      "Epoch 144, loss: 631.091868\n",
      "Epoch 145, loss: 624.726008\n",
      "Epoch 146, loss: 634.358935\n",
      "Epoch 147, loss: 613.512766\n",
      "Epoch 148, loss: 624.714058\n",
      "Epoch 149, loss: 615.718214\n",
      "Epoch 150, loss: 628.801535\n",
      "Epoch 151, loss: 610.806345\n",
      "Epoch 152, loss: 618.457668\n",
      "Epoch 153, loss: 625.757195\n",
      "Epoch 154, loss: 616.447597\n",
      "Epoch 155, loss: 623.178195\n",
      "Epoch 156, loss: 632.759507\n",
      "Epoch 157, loss: 624.579822\n",
      "Epoch 158, loss: 622.970716\n",
      "Epoch 159, loss: 603.134979\n",
      "Epoch 160, loss: 629.724655\n",
      "Epoch 161, loss: 625.224265\n",
      "Epoch 162, loss: 629.115373\n",
      "Epoch 163, loss: 619.406388\n",
      "Epoch 164, loss: 620.559343\n",
      "Epoch 165, loss: 621.784696\n",
      "Epoch 166, loss: 628.507607\n",
      "Epoch 167, loss: 612.023970\n",
      "Epoch 168, loss: 619.933458\n",
      "Epoch 169, loss: 601.663197\n",
      "Epoch 170, loss: 632.314840\n",
      "Epoch 171, loss: 613.991449\n",
      "Epoch 172, loss: 621.908266\n",
      "Epoch 173, loss: 616.903531\n",
      "Epoch 174, loss: 632.555823\n",
      "Epoch 175, loss: 615.037603\n",
      "Epoch 176, loss: 607.036715\n",
      "Epoch 177, loss: 603.555594\n",
      "Epoch 178, loss: 624.684002\n",
      "Epoch 179, loss: 622.396407\n",
      "Epoch 180, loss: 604.568148\n",
      "Epoch 181, loss: 612.304746\n",
      "Epoch 182, loss: 620.284952\n",
      "Epoch 183, loss: 621.271285\n",
      "Epoch 184, loss: 618.947105\n",
      "Epoch 185, loss: 627.020446\n",
      "Epoch 186, loss: 633.349619\n",
      "Epoch 187, loss: 622.401297\n",
      "Epoch 188, loss: 624.099270\n",
      "Epoch 189, loss: 608.176457\n",
      "Epoch 190, loss: 641.126532\n",
      "Epoch 191, loss: 606.045774\n",
      "Epoch 192, loss: 629.149793\n",
      "Epoch 193, loss: 604.602051\n",
      "Epoch 194, loss: 608.048795\n",
      "Epoch 195, loss: 615.113407\n",
      "Epoch 196, loss: 620.594240\n",
      "Epoch 197, loss: 603.251374\n",
      "Epoch 198, loss: 631.075173\n",
      "Epoch 199, loss: 608.342588\n",
      "Epoch 0, loss: 689.610601\n",
      "Epoch 1, loss: 676.679733\n",
      "Epoch 2, loss: 673.613985\n",
      "Epoch 3, loss: 671.364471\n",
      "Epoch 4, loss: 671.984940\n",
      "Epoch 5, loss: 668.172101\n",
      "Epoch 6, loss: 653.341568\n",
      "Epoch 7, loss: 665.857251\n",
      "Epoch 8, loss: 662.270361\n",
      "Epoch 9, loss: 655.412980\n",
      "Epoch 10, loss: 652.184567\n",
      "Epoch 11, loss: 652.175161\n",
      "Epoch 12, loss: 655.298640\n",
      "Epoch 13, loss: 660.617000\n",
      "Epoch 14, loss: 656.156946\n",
      "Epoch 15, loss: 652.386387\n",
      "Epoch 16, loss: 642.398897\n",
      "Epoch 17, loss: 646.114074\n",
      "Epoch 18, loss: 647.007959\n",
      "Epoch 19, loss: 642.004081\n",
      "Epoch 20, loss: 632.045653\n",
      "Epoch 21, loss: 640.255528\n",
      "Epoch 22, loss: 634.517978\n",
      "Epoch 23, loss: 644.772616\n",
      "Epoch 24, loss: 641.733222\n",
      "Epoch 25, loss: 649.438901\n",
      "Epoch 26, loss: 653.049952\n",
      "Epoch 27, loss: 643.831923\n",
      "Epoch 28, loss: 645.315691\n",
      "Epoch 29, loss: 639.508227\n",
      "Epoch 30, loss: 660.704497\n",
      "Epoch 31, loss: 641.777650\n",
      "Epoch 32, loss: 656.208880\n",
      "Epoch 33, loss: 644.481242\n",
      "Epoch 34, loss: 630.637874\n",
      "Epoch 35, loss: 630.259948\n",
      "Epoch 36, loss: 638.596882\n",
      "Epoch 37, loss: 628.329716\n",
      "Epoch 38, loss: 628.712513\n",
      "Epoch 39, loss: 644.152725\n",
      "Epoch 40, loss: 642.473632\n",
      "Epoch 41, loss: 641.417761\n",
      "Epoch 42, loss: 637.059583\n",
      "Epoch 43, loss: 643.245990\n",
      "Epoch 44, loss: 623.589647\n",
      "Epoch 45, loss: 630.708353\n",
      "Epoch 46, loss: 621.246677\n",
      "Epoch 47, loss: 660.444803\n",
      "Epoch 48, loss: 634.838562\n",
      "Epoch 49, loss: 632.809438\n",
      "Epoch 50, loss: 637.128728\n",
      "Epoch 51, loss: 633.692001\n",
      "Epoch 52, loss: 630.774738\n",
      "Epoch 53, loss: 636.652360\n",
      "Epoch 54, loss: 641.723828\n",
      "Epoch 55, loss: 646.359115\n",
      "Epoch 56, loss: 635.358229\n",
      "Epoch 57, loss: 625.649248\n",
      "Epoch 58, loss: 624.738359\n",
      "Epoch 59, loss: 642.257303\n",
      "Epoch 60, loss: 634.785623\n",
      "Epoch 61, loss: 627.853214\n",
      "Epoch 62, loss: 643.782258\n",
      "Epoch 63, loss: 644.127961\n",
      "Epoch 64, loss: 623.582893\n",
      "Epoch 65, loss: 643.008658\n",
      "Epoch 66, loss: 634.191530\n",
      "Epoch 67, loss: 656.197261\n",
      "Epoch 68, loss: 625.772612\n",
      "Epoch 69, loss: 634.120629\n",
      "Epoch 70, loss: 649.856046\n",
      "Epoch 71, loss: 630.115001\n",
      "Epoch 72, loss: 635.196186\n",
      "Epoch 73, loss: 623.753704\n",
      "Epoch 74, loss: 623.509987\n",
      "Epoch 75, loss: 618.543372\n",
      "Epoch 76, loss: 629.538722\n",
      "Epoch 77, loss: 644.048952\n",
      "Epoch 78, loss: 630.858520\n",
      "Epoch 79, loss: 629.149980\n",
      "Epoch 80, loss: 636.078678\n",
      "Epoch 81, loss: 623.236922\n",
      "Epoch 82, loss: 636.541844\n",
      "Epoch 83, loss: 629.776948\n",
      "Epoch 84, loss: 615.197840\n",
      "Epoch 85, loss: 609.491147\n",
      "Epoch 86, loss: 645.086177\n",
      "Epoch 87, loss: 635.958525\n",
      "Epoch 88, loss: 626.263717\n",
      "Epoch 89, loss: 641.962880\n",
      "Epoch 90, loss: 602.882696\n",
      "Epoch 91, loss: 615.140881\n",
      "Epoch 92, loss: 615.974858\n",
      "Epoch 93, loss: 633.112167\n",
      "Epoch 94, loss: 635.797524\n",
      "Epoch 95, loss: 626.661336\n",
      "Epoch 96, loss: 621.760727\n",
      "Epoch 97, loss: 628.462921\n",
      "Epoch 98, loss: 613.870189\n",
      "Epoch 99, loss: 640.779079\n",
      "Epoch 100, loss: 630.495241\n",
      "Epoch 101, loss: 624.507865\n",
      "Epoch 102, loss: 621.214612\n",
      "Epoch 103, loss: 636.939157\n",
      "Epoch 104, loss: 628.935947\n",
      "Epoch 105, loss: 611.357622\n",
      "Epoch 106, loss: 610.408637\n",
      "Epoch 107, loss: 611.403711\n",
      "Epoch 108, loss: 633.482915\n",
      "Epoch 109, loss: 636.186546\n",
      "Epoch 110, loss: 612.986585\n",
      "Epoch 111, loss: 627.087376\n",
      "Epoch 112, loss: 620.185580\n",
      "Epoch 113, loss: 639.238493\n",
      "Epoch 114, loss: 619.966815\n",
      "Epoch 115, loss: 630.115512\n",
      "Epoch 116, loss: 634.965719\n",
      "Epoch 117, loss: 609.773140\n",
      "Epoch 118, loss: 608.941195\n",
      "Epoch 119, loss: 641.676185\n",
      "Epoch 120, loss: 629.166275\n",
      "Epoch 121, loss: 628.688914\n",
      "Epoch 122, loss: 638.704303\n",
      "Epoch 123, loss: 621.329885\n",
      "Epoch 124, loss: 632.640052\n",
      "Epoch 125, loss: 627.016197\n",
      "Epoch 126, loss: 599.001459\n",
      "Epoch 127, loss: 647.714975\n",
      "Epoch 128, loss: 622.705055\n",
      "Epoch 129, loss: 632.416998\n",
      "Epoch 130, loss: 649.079400\n",
      "Epoch 131, loss: 627.029501\n",
      "Epoch 132, loss: 621.680936\n",
      "Epoch 133, loss: 624.012757\n",
      "Epoch 134, loss: 621.660804\n",
      "Epoch 135, loss: 619.560611\n",
      "Epoch 136, loss: 607.680268\n",
      "Epoch 137, loss: 620.454258\n",
      "Epoch 138, loss: 610.964895\n",
      "Epoch 139, loss: 636.320616\n",
      "Epoch 140, loss: 605.660547\n",
      "Epoch 141, loss: 643.073571\n",
      "Epoch 142, loss: 622.155530\n",
      "Epoch 143, loss: 624.742516\n",
      "Epoch 144, loss: 617.605229\n",
      "Epoch 145, loss: 616.446201\n",
      "Epoch 146, loss: 628.580636\n",
      "Epoch 147, loss: 641.032655\n",
      "Epoch 148, loss: 627.292793\n",
      "Epoch 149, loss: 618.508059\n",
      "Epoch 150, loss: 616.393540\n",
      "Epoch 151, loss: 617.080236\n",
      "Epoch 152, loss: 628.421941\n",
      "Epoch 153, loss: 633.333705\n",
      "Epoch 154, loss: 612.217794\n",
      "Epoch 155, loss: 621.595300\n",
      "Epoch 156, loss: 615.463777\n",
      "Epoch 157, loss: 632.423716\n",
      "Epoch 158, loss: 642.487762\n",
      "Epoch 159, loss: 622.248597\n",
      "Epoch 160, loss: 611.816074\n",
      "Epoch 161, loss: 640.845374\n",
      "Epoch 162, loss: 622.834982\n",
      "Epoch 163, loss: 605.438600\n",
      "Epoch 164, loss: 618.094191\n",
      "Epoch 165, loss: 636.156206\n",
      "Epoch 166, loss: 636.651499\n",
      "Epoch 167, loss: 628.253141\n",
      "Epoch 168, loss: 623.970307\n",
      "Epoch 169, loss: 619.364987\n",
      "Epoch 170, loss: 592.714899\n",
      "Epoch 171, loss: 631.107065\n",
      "Epoch 172, loss: 617.711108\n",
      "Epoch 173, loss: 630.295550\n",
      "Epoch 174, loss: 616.525059\n",
      "Epoch 175, loss: 615.890995\n",
      "Epoch 176, loss: 623.337069\n",
      "Epoch 177, loss: 616.820924\n",
      "Epoch 178, loss: 607.989782\n",
      "Epoch 179, loss: 632.704399\n",
      "Epoch 180, loss: 598.674241\n",
      "Epoch 181, loss: 625.295700\n",
      "Epoch 182, loss: 620.701860\n",
      "Epoch 183, loss: 640.233308\n",
      "Epoch 184, loss: 606.909852\n",
      "Epoch 185, loss: 615.677255\n",
      "Epoch 186, loss: 612.067827\n",
      "Epoch 187, loss: 624.124881\n",
      "Epoch 188, loss: 599.113208\n",
      "Epoch 189, loss: 611.447723\n",
      "Epoch 190, loss: 610.250386\n",
      "Epoch 191, loss: 592.818791\n",
      "Epoch 192, loss: 614.818652\n",
      "Epoch 193, loss: 637.309196\n",
      "Epoch 194, loss: 628.441939\n",
      "Epoch 195, loss: 638.288460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, loss: 624.186709\n",
      "Epoch 197, loss: 616.151276\n",
      "Epoch 198, loss: 610.520651\n",
      "Epoch 199, loss: 633.933314\n",
      "Epoch 0, loss: 690.234364\n",
      "Epoch 1, loss: 690.102874\n",
      "Epoch 2, loss: 689.099249\n",
      "Epoch 3, loss: 687.250700\n",
      "Epoch 4, loss: 686.581931\n",
      "Epoch 5, loss: 688.940125\n",
      "Epoch 6, loss: 685.712480\n",
      "Epoch 7, loss: 684.770960\n",
      "Epoch 8, loss: 685.549863\n",
      "Epoch 9, loss: 683.229423\n",
      "Epoch 10, loss: 682.140335\n",
      "Epoch 11, loss: 685.133695\n",
      "Epoch 12, loss: 683.039200\n",
      "Epoch 13, loss: 685.536440\n",
      "Epoch 14, loss: 679.774652\n",
      "Epoch 15, loss: 683.088802\n",
      "Epoch 16, loss: 676.959781\n",
      "Epoch 17, loss: 677.081985\n",
      "Epoch 18, loss: 678.306506\n",
      "Epoch 19, loss: 674.851815\n",
      "Epoch 20, loss: 676.814423\n",
      "Epoch 21, loss: 676.769674\n",
      "Epoch 22, loss: 675.786818\n",
      "Epoch 23, loss: 678.640288\n",
      "Epoch 24, loss: 672.771400\n",
      "Epoch 25, loss: 672.833346\n",
      "Epoch 26, loss: 671.584521\n",
      "Epoch 27, loss: 677.417657\n",
      "Epoch 28, loss: 673.946594\n",
      "Epoch 29, loss: 671.804128\n",
      "Epoch 30, loss: 669.725249\n",
      "Epoch 31, loss: 676.937247\n",
      "Epoch 32, loss: 675.736955\n",
      "Epoch 33, loss: 670.911579\n",
      "Epoch 34, loss: 673.595056\n",
      "Epoch 35, loss: 673.654250\n",
      "Epoch 36, loss: 675.793108\n",
      "Epoch 37, loss: 676.554946\n",
      "Epoch 38, loss: 667.394329\n",
      "Epoch 39, loss: 673.650474\n",
      "Epoch 40, loss: 664.963782\n",
      "Epoch 41, loss: 665.812253\n",
      "Epoch 42, loss: 668.377614\n",
      "Epoch 43, loss: 666.005479\n",
      "Epoch 44, loss: 663.693752\n",
      "Epoch 45, loss: 666.051721\n",
      "Epoch 46, loss: 660.751276\n",
      "Epoch 47, loss: 664.543907\n",
      "Epoch 48, loss: 664.494450\n",
      "Epoch 49, loss: 672.169621\n",
      "Epoch 50, loss: 660.798225\n",
      "Epoch 51, loss: 667.764805\n",
      "Epoch 52, loss: 663.461251\n",
      "Epoch 53, loss: 661.037778\n",
      "Epoch 54, loss: 668.884326\n",
      "Epoch 55, loss: 662.771224\n",
      "Epoch 56, loss: 660.922445\n",
      "Epoch 57, loss: 661.158517\n",
      "Epoch 58, loss: 657.088834\n",
      "Epoch 59, loss: 663.126705\n",
      "Epoch 60, loss: 658.767304\n",
      "Epoch 61, loss: 659.805476\n",
      "Epoch 62, loss: 658.657617\n",
      "Epoch 63, loss: 652.306840\n",
      "Epoch 64, loss: 662.187279\n",
      "Epoch 65, loss: 664.044777\n",
      "Epoch 66, loss: 658.212856\n",
      "Epoch 67, loss: 660.821253\n",
      "Epoch 68, loss: 664.403163\n",
      "Epoch 69, loss: 666.982574\n",
      "Epoch 70, loss: 661.415918\n",
      "Epoch 71, loss: 662.374385\n",
      "Epoch 72, loss: 655.385217\n",
      "Epoch 73, loss: 648.872368\n",
      "Epoch 74, loss: 650.716983\n",
      "Epoch 75, loss: 652.876785\n",
      "Epoch 76, loss: 662.003944\n",
      "Epoch 77, loss: 653.091327\n",
      "Epoch 78, loss: 647.247357\n",
      "Epoch 79, loss: 658.084321\n",
      "Epoch 80, loss: 658.436765\n",
      "Epoch 81, loss: 664.621850\n",
      "Epoch 82, loss: 658.989368\n",
      "Epoch 83, loss: 647.942102\n",
      "Epoch 84, loss: 652.734147\n",
      "Epoch 85, loss: 659.867766\n",
      "Epoch 86, loss: 666.035580\n",
      "Epoch 87, loss: 658.658770\n",
      "Epoch 88, loss: 656.716719\n",
      "Epoch 89, loss: 652.744430\n",
      "Epoch 90, loss: 652.504859\n",
      "Epoch 91, loss: 647.519198\n",
      "Epoch 92, loss: 654.345694\n",
      "Epoch 93, loss: 662.599090\n",
      "Epoch 94, loss: 666.340711\n",
      "Epoch 95, loss: 657.663272\n",
      "Epoch 96, loss: 656.186386\n",
      "Epoch 97, loss: 651.256079\n",
      "Epoch 98, loss: 654.471480\n",
      "Epoch 99, loss: 639.655642\n",
      "Epoch 100, loss: 656.200809\n",
      "Epoch 101, loss: 654.854870\n",
      "Epoch 102, loss: 657.418338\n",
      "Epoch 103, loss: 656.793831\n",
      "Epoch 104, loss: 651.550926\n",
      "Epoch 105, loss: 647.663773\n",
      "Epoch 106, loss: 653.229958\n",
      "Epoch 107, loss: 646.843388\n",
      "Epoch 108, loss: 653.577212\n",
      "Epoch 109, loss: 660.947774\n",
      "Epoch 110, loss: 667.644689\n",
      "Epoch 111, loss: 650.912266\n",
      "Epoch 112, loss: 636.406948\n",
      "Epoch 113, loss: 649.376852\n",
      "Epoch 114, loss: 653.012980\n",
      "Epoch 115, loss: 641.253415\n",
      "Epoch 116, loss: 654.138973\n",
      "Epoch 117, loss: 644.179446\n",
      "Epoch 118, loss: 665.492117\n",
      "Epoch 119, loss: 648.941705\n",
      "Epoch 120, loss: 651.197042\n",
      "Epoch 121, loss: 651.156050\n",
      "Epoch 122, loss: 652.971136\n",
      "Epoch 123, loss: 647.497754\n",
      "Epoch 124, loss: 648.007755\n",
      "Epoch 125, loss: 654.185830\n",
      "Epoch 126, loss: 661.452249\n",
      "Epoch 127, loss: 651.048233\n",
      "Epoch 128, loss: 650.863587\n",
      "Epoch 129, loss: 660.665611\n",
      "Epoch 130, loss: 646.024508\n",
      "Epoch 131, loss: 659.979004\n",
      "Epoch 132, loss: 652.052297\n",
      "Epoch 133, loss: 650.086424\n",
      "Epoch 134, loss: 644.622463\n",
      "Epoch 135, loss: 645.334096\n",
      "Epoch 136, loss: 655.462869\n",
      "Epoch 137, loss: 666.586622\n",
      "Epoch 138, loss: 650.857042\n",
      "Epoch 139, loss: 648.315274\n",
      "Epoch 140, loss: 654.755739\n",
      "Epoch 141, loss: 647.641796\n",
      "Epoch 142, loss: 649.969700\n",
      "Epoch 143, loss: 646.452017\n",
      "Epoch 144, loss: 650.742339\n",
      "Epoch 145, loss: 643.200687\n",
      "Epoch 146, loss: 653.915093\n",
      "Epoch 147, loss: 647.129998\n",
      "Epoch 148, loss: 639.144352\n",
      "Epoch 149, loss: 656.881954\n",
      "Epoch 150, loss: 637.541958\n",
      "Epoch 151, loss: 648.961888\n",
      "Epoch 152, loss: 651.262447\n",
      "Epoch 153, loss: 649.196433\n",
      "Epoch 154, loss: 643.465121\n",
      "Epoch 155, loss: 655.236177\n",
      "Epoch 156, loss: 644.213578\n",
      "Epoch 157, loss: 653.555055\n",
      "Epoch 158, loss: 652.736736\n",
      "Epoch 159, loss: 642.280962\n",
      "Epoch 160, loss: 632.492956\n",
      "Epoch 161, loss: 648.451833\n",
      "Epoch 162, loss: 656.792786\n",
      "Epoch 163, loss: 655.086150\n",
      "Epoch 164, loss: 644.224588\n",
      "Epoch 165, loss: 633.237280\n",
      "Epoch 166, loss: 656.510569\n",
      "Epoch 167, loss: 639.438518\n",
      "Epoch 168, loss: 641.562999\n",
      "Epoch 169, loss: 646.736563\n",
      "Epoch 170, loss: 652.656352\n",
      "Epoch 171, loss: 638.466712\n",
      "Epoch 172, loss: 642.948154\n",
      "Epoch 173, loss: 641.718785\n",
      "Epoch 174, loss: 650.704627\n",
      "Epoch 175, loss: 652.302732\n",
      "Epoch 176, loss: 643.301126\n",
      "Epoch 177, loss: 654.093541\n",
      "Epoch 178, loss: 634.170096\n",
      "Epoch 179, loss: 648.663778\n",
      "Epoch 180, loss: 649.906634\n",
      "Epoch 181, loss: 650.699465\n",
      "Epoch 182, loss: 652.466098\n",
      "Epoch 183, loss: 634.093382\n",
      "Epoch 184, loss: 665.640970\n",
      "Epoch 185, loss: 644.218643\n",
      "Epoch 186, loss: 639.021378\n",
      "Epoch 187, loss: 641.265089\n",
      "Epoch 188, loss: 638.017803\n",
      "Epoch 189, loss: 653.521759\n",
      "Epoch 190, loss: 636.439743\n",
      "Epoch 191, loss: 646.587129\n",
      "Epoch 192, loss: 651.498403\n",
      "Epoch 193, loss: 646.197839\n",
      "Epoch 194, loss: 640.614421\n",
      "Epoch 195, loss: 642.158294\n",
      "Epoch 196, loss: 648.862457\n",
      "Epoch 197, loss: 646.685861\n",
      "Epoch 198, loss: 667.492409\n",
      "Epoch 199, loss: 640.749155\n",
      "Epoch 0, loss: 690.829392\n",
      "Epoch 1, loss: 689.090443\n",
      "Epoch 2, loss: 689.193219\n",
      "Epoch 3, loss: 688.404227\n",
      "Epoch 4, loss: 686.820948\n",
      "Epoch 5, loss: 686.439204\n",
      "Epoch 6, loss: 683.432231\n",
      "Epoch 7, loss: 684.530299\n",
      "Epoch 8, loss: 684.889993\n",
      "Epoch 9, loss: 682.828763\n",
      "Epoch 10, loss: 681.758121\n",
      "Epoch 11, loss: 684.821265\n",
      "Epoch 12, loss: 680.730849\n",
      "Epoch 13, loss: 680.078744\n",
      "Epoch 14, loss: 680.578245\n",
      "Epoch 15, loss: 679.758397\n",
      "Epoch 16, loss: 681.152696\n",
      "Epoch 17, loss: 678.929288\n",
      "Epoch 18, loss: 677.125753\n",
      "Epoch 19, loss: 678.129168\n",
      "Epoch 20, loss: 674.775602\n",
      "Epoch 21, loss: 676.556525\n",
      "Epoch 22, loss: 674.957855\n",
      "Epoch 23, loss: 680.953875\n",
      "Epoch 24, loss: 676.774334\n",
      "Epoch 25, loss: 673.479157\n",
      "Epoch 26, loss: 677.109580\n",
      "Epoch 27, loss: 674.564620\n",
      "Epoch 28, loss: 673.977614\n",
      "Epoch 29, loss: 669.051363\n",
      "Epoch 30, loss: 671.332219\n",
      "Epoch 31, loss: 676.039087\n",
      "Epoch 32, loss: 668.319845\n",
      "Epoch 33, loss: 672.002036\n",
      "Epoch 34, loss: 669.694416\n",
      "Epoch 35, loss: 672.414653\n",
      "Epoch 36, loss: 669.437482\n",
      "Epoch 37, loss: 670.439280\n",
      "Epoch 38, loss: 671.912665\n",
      "Epoch 39, loss: 671.830625\n",
      "Epoch 40, loss: 669.585276\n",
      "Epoch 41, loss: 674.888467\n",
      "Epoch 42, loss: 670.298333\n",
      "Epoch 43, loss: 667.597022\n",
      "Epoch 44, loss: 665.716050\n",
      "Epoch 45, loss: 664.418281\n",
      "Epoch 46, loss: 656.723955\n",
      "Epoch 47, loss: 663.393416\n",
      "Epoch 48, loss: 671.296147\n",
      "Epoch 49, loss: 667.921091\n",
      "Epoch 50, loss: 661.472952\n",
      "Epoch 51, loss: 659.829897\n",
      "Epoch 52, loss: 666.190543\n",
      "Epoch 53, loss: 658.741442\n",
      "Epoch 54, loss: 671.923622\n",
      "Epoch 55, loss: 663.587756\n",
      "Epoch 56, loss: 668.258621\n",
      "Epoch 57, loss: 670.193534\n",
      "Epoch 58, loss: 664.803765\n",
      "Epoch 59, loss: 662.079939\n",
      "Epoch 60, loss: 666.928610\n",
      "Epoch 61, loss: 668.564922\n",
      "Epoch 62, loss: 659.887921\n",
      "Epoch 63, loss: 662.934191\n",
      "Epoch 64, loss: 660.772291\n",
      "Epoch 65, loss: 656.178107\n",
      "Epoch 66, loss: 653.210129\n",
      "Epoch 67, loss: 665.803839\n",
      "Epoch 68, loss: 663.381859\n",
      "Epoch 69, loss: 661.158998\n",
      "Epoch 70, loss: 656.218926\n",
      "Epoch 71, loss: 668.287636\n",
      "Epoch 72, loss: 645.523474\n",
      "Epoch 73, loss: 661.896146\n",
      "Epoch 74, loss: 666.811562\n",
      "Epoch 75, loss: 661.414155\n",
      "Epoch 76, loss: 650.857564\n",
      "Epoch 77, loss: 661.357793\n",
      "Epoch 78, loss: 663.368504\n",
      "Epoch 79, loss: 649.270332\n",
      "Epoch 80, loss: 658.889121\n",
      "Epoch 81, loss: 660.231831\n",
      "Epoch 82, loss: 653.139837\n",
      "Epoch 83, loss: 653.372148\n",
      "Epoch 84, loss: 661.349047\n",
      "Epoch 85, loss: 654.845585\n",
      "Epoch 86, loss: 652.084482\n",
      "Epoch 87, loss: 664.889867\n",
      "Epoch 88, loss: 650.872417\n",
      "Epoch 89, loss: 661.720124\n",
      "Epoch 90, loss: 658.967325\n",
      "Epoch 91, loss: 664.957098\n",
      "Epoch 92, loss: 658.232597\n",
      "Epoch 93, loss: 656.859565\n",
      "Epoch 94, loss: 654.575726\n",
      "Epoch 95, loss: 659.896755\n",
      "Epoch 96, loss: 654.816517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, loss: 638.354315\n",
      "Epoch 98, loss: 659.724624\n",
      "Epoch 99, loss: 653.837858\n",
      "Epoch 100, loss: 647.962096\n",
      "Epoch 101, loss: 649.645937\n",
      "Epoch 102, loss: 648.136539\n",
      "Epoch 103, loss: 655.828154\n",
      "Epoch 104, loss: 662.747542\n",
      "Epoch 105, loss: 656.343591\n",
      "Epoch 106, loss: 656.504888\n",
      "Epoch 107, loss: 655.452391\n",
      "Epoch 108, loss: 653.188140\n",
      "Epoch 109, loss: 650.420805\n",
      "Epoch 110, loss: 648.576637\n",
      "Epoch 111, loss: 646.734932\n",
      "Epoch 112, loss: 655.102057\n",
      "Epoch 113, loss: 652.527757\n",
      "Epoch 114, loss: 645.333763\n",
      "Epoch 115, loss: 646.908355\n",
      "Epoch 116, loss: 649.089059\n",
      "Epoch 117, loss: 657.431082\n",
      "Epoch 118, loss: 648.653677\n",
      "Epoch 119, loss: 651.759426\n",
      "Epoch 120, loss: 651.983122\n",
      "Epoch 121, loss: 659.296193\n",
      "Epoch 122, loss: 654.278165\n",
      "Epoch 123, loss: 654.146252\n",
      "Epoch 124, loss: 648.600348\n",
      "Epoch 125, loss: 650.816517\n",
      "Epoch 126, loss: 650.347602\n",
      "Epoch 127, loss: 644.085152\n",
      "Epoch 128, loss: 651.320704\n",
      "Epoch 129, loss: 645.329509\n",
      "Epoch 130, loss: 653.288827\n",
      "Epoch 131, loss: 650.073832\n",
      "Epoch 132, loss: 649.487267\n",
      "Epoch 133, loss: 654.333438\n",
      "Epoch 134, loss: 646.235203\n",
      "Epoch 135, loss: 656.785733\n",
      "Epoch 136, loss: 661.513186\n",
      "Epoch 137, loss: 657.420664\n",
      "Epoch 138, loss: 654.037743\n",
      "Epoch 139, loss: 653.894169\n",
      "Epoch 140, loss: 647.690858\n",
      "Epoch 141, loss: 653.816558\n",
      "Epoch 142, loss: 643.427726\n",
      "Epoch 143, loss: 646.255052\n",
      "Epoch 144, loss: 655.682127\n",
      "Epoch 145, loss: 645.071121\n",
      "Epoch 146, loss: 652.400559\n",
      "Epoch 147, loss: 644.938815\n",
      "Epoch 148, loss: 639.413544\n",
      "Epoch 149, loss: 639.237345\n",
      "Epoch 150, loss: 657.197921\n",
      "Epoch 151, loss: 640.471251\n",
      "Epoch 152, loss: 647.490974\n",
      "Epoch 153, loss: 648.458757\n",
      "Epoch 154, loss: 647.288715\n",
      "Epoch 155, loss: 648.709252\n",
      "Epoch 156, loss: 651.443487\n",
      "Epoch 157, loss: 654.982063\n",
      "Epoch 158, loss: 646.002663\n",
      "Epoch 159, loss: 648.167546\n",
      "Epoch 160, loss: 653.587270\n",
      "Epoch 161, loss: 639.966052\n",
      "Epoch 162, loss: 647.199277\n",
      "Epoch 163, loss: 628.191976\n",
      "Epoch 164, loss: 650.744843\n",
      "Epoch 165, loss: 647.924092\n",
      "Epoch 166, loss: 646.477977\n",
      "Epoch 167, loss: 634.052544\n",
      "Epoch 168, loss: 644.554695\n",
      "Epoch 169, loss: 640.511770\n",
      "Epoch 170, loss: 646.031317\n",
      "Epoch 171, loss: 644.893771\n",
      "Epoch 172, loss: 644.484176\n",
      "Epoch 173, loss: 644.630945\n",
      "Epoch 174, loss: 637.282312\n",
      "Epoch 175, loss: 652.446665\n",
      "Epoch 176, loss: 644.673682\n",
      "Epoch 177, loss: 648.027693\n",
      "Epoch 178, loss: 636.774988\n",
      "Epoch 179, loss: 653.663785\n",
      "Epoch 180, loss: 635.301885\n",
      "Epoch 181, loss: 643.604348\n",
      "Epoch 182, loss: 638.131746\n",
      "Epoch 183, loss: 649.234092\n",
      "Epoch 184, loss: 655.660939\n",
      "Epoch 185, loss: 634.886131\n",
      "Epoch 186, loss: 639.011101\n",
      "Epoch 187, loss: 644.099278\n",
      "Epoch 188, loss: 648.632618\n",
      "Epoch 189, loss: 650.104257\n",
      "Epoch 190, loss: 637.131497\n",
      "Epoch 191, loss: 643.870699\n",
      "Epoch 192, loss: 651.536141\n",
      "Epoch 193, loss: 654.346584\n",
      "Epoch 194, loss: 631.883706\n",
      "Epoch 195, loss: 639.900800\n",
      "Epoch 196, loss: 634.201629\n",
      "Epoch 197, loss: 653.656502\n",
      "Epoch 198, loss: 645.853389\n",
      "Epoch 199, loss: 653.357141\n",
      "Epoch 0, loss: 690.140906\n",
      "Epoch 1, loss: 690.001385\n",
      "Epoch 2, loss: 688.530973\n",
      "Epoch 3, loss: 688.657013\n",
      "Epoch 4, loss: 687.511725\n",
      "Epoch 5, loss: 686.426139\n",
      "Epoch 6, loss: 685.605054\n",
      "Epoch 7, loss: 685.375631\n",
      "Epoch 8, loss: 683.658169\n",
      "Epoch 9, loss: 682.894745\n",
      "Epoch 10, loss: 684.332251\n",
      "Epoch 11, loss: 682.843375\n",
      "Epoch 12, loss: 682.673463\n",
      "Epoch 13, loss: 682.297984\n",
      "Epoch 14, loss: 679.648205\n",
      "Epoch 15, loss: 678.723807\n",
      "Epoch 16, loss: 677.419450\n",
      "Epoch 17, loss: 679.330708\n",
      "Epoch 18, loss: 675.693295\n",
      "Epoch 19, loss: 676.260106\n",
      "Epoch 20, loss: 677.543766\n",
      "Epoch 21, loss: 676.949587\n",
      "Epoch 22, loss: 672.215145\n",
      "Epoch 23, loss: 674.614263\n",
      "Epoch 24, loss: 674.086057\n",
      "Epoch 25, loss: 672.848760\n",
      "Epoch 26, loss: 675.565549\n",
      "Epoch 27, loss: 678.258142\n",
      "Epoch 28, loss: 677.608086\n",
      "Epoch 29, loss: 667.451889\n",
      "Epoch 30, loss: 674.241090\n",
      "Epoch 31, loss: 668.134204\n",
      "Epoch 32, loss: 673.546986\n",
      "Epoch 33, loss: 676.720405\n",
      "Epoch 34, loss: 666.885072\n",
      "Epoch 35, loss: 671.161143\n",
      "Epoch 36, loss: 665.577740\n",
      "Epoch 37, loss: 672.142130\n",
      "Epoch 38, loss: 670.653000\n",
      "Epoch 39, loss: 671.973180\n",
      "Epoch 40, loss: 666.357086\n",
      "Epoch 41, loss: 666.956387\n",
      "Epoch 42, loss: 664.011193\n",
      "Epoch 43, loss: 667.272226\n",
      "Epoch 44, loss: 671.904007\n",
      "Epoch 45, loss: 667.908835\n",
      "Epoch 46, loss: 668.213442\n",
      "Epoch 47, loss: 672.065758\n",
      "Epoch 48, loss: 657.240998\n",
      "Epoch 49, loss: 660.867481\n",
      "Epoch 50, loss: 666.224504\n",
      "Epoch 51, loss: 663.550526\n",
      "Epoch 52, loss: 669.625979\n",
      "Epoch 53, loss: 661.957694\n",
      "Epoch 54, loss: 660.030341\n",
      "Epoch 55, loss: 666.047829\n",
      "Epoch 56, loss: 663.800647\n",
      "Epoch 57, loss: 672.395340\n",
      "Epoch 58, loss: 670.823630\n",
      "Epoch 59, loss: 671.966928\n",
      "Epoch 60, loss: 657.803225\n",
      "Epoch 61, loss: 663.979919\n",
      "Epoch 62, loss: 667.041631\n",
      "Epoch 63, loss: 664.094454\n",
      "Epoch 64, loss: 665.676451\n",
      "Epoch 65, loss: 655.205452\n",
      "Epoch 66, loss: 661.116531\n",
      "Epoch 67, loss: 662.963027\n",
      "Epoch 68, loss: 660.958066\n",
      "Epoch 69, loss: 667.679300\n",
      "Epoch 70, loss: 665.884632\n",
      "Epoch 71, loss: 657.716595\n",
      "Epoch 72, loss: 654.110778\n",
      "Epoch 73, loss: 657.905438\n",
      "Epoch 74, loss: 646.510964\n",
      "Epoch 75, loss: 646.453897\n",
      "Epoch 76, loss: 660.792072\n",
      "Epoch 77, loss: 650.521721\n",
      "Epoch 78, loss: 664.436194\n",
      "Epoch 79, loss: 659.255190\n",
      "Epoch 80, loss: 652.462584\n",
      "Epoch 81, loss: 661.060567\n",
      "Epoch 82, loss: 652.858237\n",
      "Epoch 83, loss: 658.098256\n",
      "Epoch 84, loss: 658.798833\n",
      "Epoch 85, loss: 664.653547\n",
      "Epoch 86, loss: 655.695041\n",
      "Epoch 87, loss: 653.127039\n",
      "Epoch 88, loss: 658.367733\n",
      "Epoch 89, loss: 652.269030\n",
      "Epoch 90, loss: 645.326220\n",
      "Epoch 91, loss: 650.434857\n",
      "Epoch 92, loss: 655.571445\n",
      "Epoch 93, loss: 652.104840\n",
      "Epoch 94, loss: 657.508287\n",
      "Epoch 95, loss: 656.418771\n",
      "Epoch 96, loss: 659.287404\n",
      "Epoch 97, loss: 650.204301\n",
      "Epoch 98, loss: 657.876039\n",
      "Epoch 99, loss: 643.935561\n",
      "Epoch 100, loss: 651.183700\n",
      "Epoch 101, loss: 651.769811\n",
      "Epoch 102, loss: 648.872232\n",
      "Epoch 103, loss: 660.012167\n",
      "Epoch 104, loss: 656.099878\n",
      "Epoch 105, loss: 656.326041\n",
      "Epoch 106, loss: 649.132811\n",
      "Epoch 107, loss: 651.869638\n",
      "Epoch 108, loss: 637.143221\n",
      "Epoch 109, loss: 651.454416\n",
      "Epoch 110, loss: 669.267466\n",
      "Epoch 111, loss: 649.361623\n",
      "Epoch 112, loss: 651.019287\n",
      "Epoch 113, loss: 654.217204\n",
      "Epoch 114, loss: 649.389804\n",
      "Epoch 115, loss: 665.732193\n",
      "Epoch 116, loss: 652.870762\n",
      "Epoch 117, loss: 650.796916\n",
      "Epoch 118, loss: 659.116503\n",
      "Epoch 119, loss: 644.961961\n",
      "Epoch 120, loss: 656.568322\n",
      "Epoch 121, loss: 643.564037\n",
      "Epoch 122, loss: 657.699403\n",
      "Epoch 123, loss: 659.715973\n",
      "Epoch 124, loss: 650.658502\n",
      "Epoch 125, loss: 658.300734\n",
      "Epoch 126, loss: 652.712700\n",
      "Epoch 127, loss: 655.159022\n",
      "Epoch 128, loss: 648.968331\n",
      "Epoch 129, loss: 654.492417\n",
      "Epoch 130, loss: 655.223155\n",
      "Epoch 131, loss: 652.577561\n",
      "Epoch 132, loss: 646.178224\n",
      "Epoch 133, loss: 646.089323\n",
      "Epoch 134, loss: 649.977698\n",
      "Epoch 135, loss: 652.497713\n",
      "Epoch 136, loss: 655.920298\n",
      "Epoch 137, loss: 654.217353\n",
      "Epoch 138, loss: 639.924551\n",
      "Epoch 139, loss: 650.467793\n",
      "Epoch 140, loss: 645.654392\n",
      "Epoch 141, loss: 665.663228\n",
      "Epoch 142, loss: 655.628417\n",
      "Epoch 143, loss: 661.407323\n",
      "Epoch 144, loss: 647.955481\n",
      "Epoch 145, loss: 645.924609\n",
      "Epoch 146, loss: 654.363537\n",
      "Epoch 147, loss: 657.980189\n",
      "Epoch 148, loss: 646.528128\n",
      "Epoch 149, loss: 656.040587\n",
      "Epoch 150, loss: 645.678307\n",
      "Epoch 151, loss: 659.699276\n",
      "Epoch 152, loss: 649.897633\n",
      "Epoch 153, loss: 642.548679\n",
      "Epoch 154, loss: 662.361377\n",
      "Epoch 155, loss: 643.217112\n",
      "Epoch 156, loss: 645.692643\n",
      "Epoch 157, loss: 636.877899\n",
      "Epoch 158, loss: 639.447014\n",
      "Epoch 159, loss: 644.170627\n",
      "Epoch 160, loss: 639.251851\n",
      "Epoch 161, loss: 666.723868\n",
      "Epoch 162, loss: 648.915355\n",
      "Epoch 163, loss: 635.177405\n",
      "Epoch 164, loss: 656.952028\n",
      "Epoch 165, loss: 654.137607\n",
      "Epoch 166, loss: 648.811577\n",
      "Epoch 167, loss: 653.658505\n",
      "Epoch 168, loss: 645.975932\n",
      "Epoch 169, loss: 651.229284\n",
      "Epoch 170, loss: 645.789262\n",
      "Epoch 171, loss: 631.673196\n",
      "Epoch 172, loss: 645.909849\n",
      "Epoch 173, loss: 637.126897\n",
      "Epoch 174, loss: 649.925499\n",
      "Epoch 175, loss: 646.514262\n",
      "Epoch 176, loss: 656.023207\n",
      "Epoch 177, loss: 646.095061\n",
      "Epoch 178, loss: 646.112663\n",
      "Epoch 179, loss: 656.324818\n",
      "Epoch 180, loss: 648.085884\n",
      "Epoch 181, loss: 646.979181\n",
      "Epoch 182, loss: 658.874446\n",
      "Epoch 183, loss: 649.243072\n",
      "Epoch 184, loss: 651.678730\n",
      "Epoch 185, loss: 642.381264\n",
      "Epoch 186, loss: 634.346201\n",
      "Epoch 187, loss: 644.674095\n",
      "Epoch 188, loss: 640.221099\n",
      "Epoch 189, loss: 658.882159\n",
      "Epoch 190, loss: 636.398249\n",
      "Epoch 191, loss: 633.653121\n",
      "Epoch 192, loss: 644.564764\n",
      "Epoch 193, loss: 647.360637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 640.118842\n",
      "Epoch 195, loss: 664.321458\n",
      "Epoch 196, loss: 629.641815\n",
      "Epoch 197, loss: 637.658313\n",
      "Epoch 198, loss: 644.958783\n",
      "Epoch 199, loss: 652.478484\n",
      "best validation accuracy achieved: 0.187000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for reg_strength in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, batch_size, learning_rate, reg_strength, num_epochs)\n",
    "        prediction = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(prediction, val_y)\n",
    "        \n",
    "        if best_val_accuracy is None or best_val_accuracy > accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.182000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
